\section*{\tb{\begin{footnotesize}SHORT TITLE:\end{footnotesize}\\CONGA: Conservational AI for Great Apes}}
\section*{\tb{\begin{footnotesize}LONG TITLE:\end{footnotesize} \\Conservational AI for Integrated Monitoring of Great Apes in the Wild}}

\tb{\textbf{100 WORD LAY SUMMARY:}\\Today, all the world’s great apes are endangered or critically endangered and in urgent need of new, more accurate, agile, and lower-latency conservation methodologies. Leveraging latest artificial intelligence advances, this project will take a unique and radically new approach by developing sensor-to-abundance AIs fully integrated directly inside today’s working great ape conservation pipelines – for estimation and forecast. We propose to bring our world-leading interdisciplinary expertise in constructing animal biometric AI systems together with massive annotation-rich camera trap data, the UK’s largest supercomputer capabilities and international ecological/statistical proficiency to build transferable next-generation end-to-end conservation technology benefitting these charismatic flagship species.}\\ \ 

\textbf{\tb{1000 WORD OVERVIEW:}}\\ \ \\
\tb{\textbf{1) EDGE OF EXTINCTION -- Motivation, Timeliness, \& Importance}.\\
Global animal diversity is declining at unprecedented rates~\cite{Tuia2022} -- orders-of-magnitude faster than in tens of millions of years~\cite{Ceballos2020}. In particular, all great ape species are now listed as Endangered or Critically Endangered, and all but one subspecies, are still declining in numbers~\cite{Hockings2015}. Despite substantial investment in conservation efforts~\cite{Bettinger2021}, western chimpanzees have declined by 80\%+ in the last two decades~\cite{Kuhl2017} having been extirpated from at least three countries (Benin, Burkina Faso and Togo)~\cite{Ginn2013,Campbell2015} and several other populations are on the edge (e.g., Ghana)~\cite{Kuhl2017}. Given these are effective flagship, umbrella, indicator and keystone species, are essential for human well-being and survival~\cite{MEA2005}, and of intense scientific interest to many scientific disciplines (e.g., biology, anthropology, psychology and their many subfields), there is a critical need -- but also untapped AI technology potential~\cite{Tuia2022} -- to protect them more effectively at this point in time~\cite{Paine1969,Tutin1991,Clucas2008}.}\\ 

\tb{\textbf{2) END-TO-END CONSERVATIONAL AI -- Background, Significance, \& Research Vision}.\\
Despite considerable research attention~\cite{Marshall2016}, relatively little effort is focused on actual conservation ~\cite{Bezanson2019}. Alarmingly, there is limited scientific evidence for the effectiveness of existing conservation strategies~\cite{Junker2020,Junker2017} driven by inability to assess population responses to conservation measures due to the absence of robust monitoring systems~\cite{Witmer2005,Junker2020}. This would require rapid  processing of sensor data to quantify shifts in species distribution and enable timely interventions. Despite pressing necessity, such systems have yet to be developed partly explaining why truly up-to-date population estimates are largely outstanding for great apes~\cite{Campbell2008, Walsh2008, Plumptre2010, Morgan2011, ArcusFoundation2014, Humle2016}. Camera traps have now been widely adopted for studying great apes~\cite{Burton2015, Tuia2022}. While easing data availability, the sheer video volumes generated have created a severe data processing bottleneck. So far, laborious expert annotations are routinely required to fuel primary 'marked' or 'unmarked' statistical frameworks that derive population estimates. In fact, the generation of video footage now significantly outpaces downstream processes needed to analyse it~\cite{Hebblewhite2010, Burton2015, Tuia2022, Pollock2025}. As a result, automated methods for timely data processing are now amongst the largest emerging needs of ecologists in general~\cite{Nathan2022} and great ape conservationists in particular~\cite{}. Moreover, models are often not integrated or even evaluated against actual ecological conservation pipelines. Instead, AI species classification, individual identification, distance estimation, and behaviour extraction are performed out of context - no system currently exists that combines these aspects to fuel statistical methods robustly for fast and accuracy-enhanced species distribution estimates. This study addresses the above gaps by attempting to build a first end-to-end AI system that generates species distribution data directly from raw camera trap videos, leveraging deep learning and computer vision in its ecological application space.} \\ \ \\
\textbf{Concept, \& Originality: Towards Sensor-To-Abundance AI Integration}.\\
\textbf{Staged Project Design: Solid Target with Blue Sky Potential}.\\
\textbf{Why The Leverhulme Trust: Fit of Remit and Ethos}.

\newpage

\section*{\tb{Why the Leverhulme? (250 words)}}
\tb{To build and transform our understanding of how humanity’s latest great invention (Artificial Intelligence) can protect biodiversity and help safeguard the practical survival of our closest relatives (great apes) sits at the heart of both, this proposal as well as my overall research vision. I believe, given the potential transferability of the project concept to countless other species, the research has potential to pioneer a new sensor-to-abundance avenue for biodiversity research and its general methodologies. Concentrating on a component-based, engineering-centred approach and one endangered family of primates gives the project focus and allows combining tangible, conservation-relevant outcomes with aspects of a radical research agenda within one project. It brings together data and methods from a diverse range of fields spanning the computational, physical and biological sciences, including deep learning and computer vision, methods in ecology and statistical population modelling, innovative sensing, imageomics, as well as traditional species conservation – all areas in which I have published. This broad blend of disciplines forms the testbed for the proposed novel end-to-end approach for AI-driven species conservation and aims to inspire future research in computational and global change biology. This project therefore represents a unique opportunity to support a truly interdisciplinary researcher and his team to rethink how AI should be evolving, and how it can develop maximum impact for good to help us better monitor and protect the natural world. For these reasons I believe that this work represents an excellent fit to the remit, mission, and ethos of the Leverhulme Trust.}

In assessing applications for funding, we use the following established criteria to prioritise work of outstanding scholarship:

\begin{itemize}
    \item Originality – the research achieves more than the incremental development of a single discipline
    \item Importance – the work will enable further research or enquiry
    \item Significance – the proposed research has relevance outside a single field, and is able to excite those working in other disciplines
    \item Merit – the quality of the research design and methodology, and the suitability of the researchers and institution for the realisation of the proposed research objectives
\end{itemize}

A second set of criteria reflect the particular values of the Leverhulme Trust, and express the Trust Board’s aspiration that our funding maintains a distinctive role within the research funding landscape. We particularly welcome applications that:

\begin{itemize}
    \item Reflect an individual’s personal vision, aspiration, or intellectual curiosity
    \item Take appropriate risks in setting and pursuing research objectives 
    \item Enable a refreshing departure from established patterns of working – either for the individual, or for the discipline
    \item Transcend disciplinary boundaries
\end{itemize}




\newpage

\section*{Conservational AI for Integrated Monitoring of Great Apes}

%\section{Introduction}
%\textbf{The Edge of Extinction: Primates \& Great Apes}. Animal diversity is declining at an unprecedented rate~\cite{Tuia2022}, with species extinction rates estimated to be hundreds or thousands of times faster than the background rates prevailing in the last tens of millions of years~\cite{Ceballos2020}. Primates are at risk throughout the world, with 60\% of all constituent species classified as being threatened with extinction by the International Union for Conservation of Nature (IUCN)~\cite{Bezanson2019}. In particular, all great ape species and subspecies are listed as Endangered or Critically Endangered, and all but one subspecies, are declining in numbers~\cite{Hockings2015} owing to habitat loss~\cite{Palminteri2018}, poaching~\cite{Campbell2008}, the pet trade~\cite{ArcusFoundation2020}, and diseases transmitted because of increased human-wildlife contact~\cite{Kondgen2008}. The western chimpanzees are critically endangered and have declined by 80\% in the last two decades~\cite{Kuhl2017}. They have already been extirpated from at least three countries (Benin, Burkina Faso and Togo)~\cite{Ginn2013,Campbell2015} and several other populations are on the edge (e.g., Ghana)~\cite{Kuhl2017}. Given they are effective flagship, umbrella and keystone species, are essential for human well-being and survival~\cite{MEA2005}, and of intense scientific interest to many disciplines (e.g., biology and its many subfields), there is a critical need to protect them~\cite{Paine1969,Tutin1991,Clucas2008}.%\textbf{Conservation Strategies Lack Quantitative Evaluation}. Although great apes attract considerable research attention—largely due to their charisma and anthropological significance~\cite{Marshall2016}—relatively little of this research is focused on conservation~\cite{Bezanson2019}. Alarmingly, there is limited scientific evidence supporting the effectiveness of existing conservation strategies~\cite{Junker2020}. Fewer than half have been evaluated quantitatively, and most remain of unknown effectiveness~\cite{Junker2017}. The inability to assess population responses to conservation measures—and, by extension, to evaluate their effectiveness—largely stems from the absence of robust monitoring systems~\cite{Witmer2005,Junker2020}. Effective monitoring requires not only the consistent collection of data, but also the capacity to process it rapidly enough to quantify shifts in species distribution—such as changes in abundance and density—soon after interventions are implemented. Despite its necessity, such systems have yet to be developed. Deficits in monitoring and evaluation of conservation at least partly explain why up-to-date estimates of distribution are currently missing for most populations of great apes~\cite{Campbell2008, Walsh2008, Plumptre2010, Morgan2011, ArcusFoundation2014, Humle2016} and, despite substantial investment in conservation efforts~\cite{Bettinger2021}, populations are projected to decline by an additional 50\% by the end of the century~\cite{}.
%\textbf{Barriers to Effective Monitoring: The Analysis Bottleneck}. Camera traps, affordable remote sensors, have been widely adopted for studying great apes~\cite{Burton2015}. While this technology has eased the burden of data acquisition, the sheer volume of data generated has created a severe data procesing bottleneck. Generating the inputs for ecological analysis requires laborious expert annotations. The primary statistical frameworks used to estimate species distribution from this data fall into two main categories: ’marked’ methods, which rely on the unique identification of individuals~\cite{}, and ’unmarked’ methods, which rely on distance~\cite{}. In addition to ’standard’ information (e.g., precise taxonmoic categorisation and behaviour) both approaches demand significant, time-consuming manual effort to extract the necessary information (e.g., individual identification and animal-camera distances) from the raw footage. It is widely recognised that the generation of camera trap video footage now significantly outpaces downstream processes needed to analyse it~\cite{Hebblewhite2010, Burton2015, Tuia2022, Pollock2025}. As a result, automated methods for processing data are now amongst the largest emerging needs of ecologists~\cite{Nathan2022}.
%\textbf{Next-generation Conservation with Deep Learning \& Computer Vision}. Deep learning and computer vision offer significant potential to expedite labor-intensive annotation tasks, which currently require expert input~\cite{Tuia2022,Pollock2025}. Although recent methods have shown promising results on ecologically relevant tasks~\cite{}, most have been evaluated outside the context of their intended applications. For example, species classification models are typically assessed using standard accuracy metrics rather than their impact on ecological measures like species occupancy, and re-identification models are judged by rank-1 statistics rather than their ability to identify all individuals in a population. As a result, these models are often evaluated on proxy tasks, limiting our understanding of how well they perform in real-world ecological pipelines. Moreover, these pipelines often require multiple outputs—such as species classification, individual identification, and behavior—necessitating the use of model ensembles. Yet, no integrated system currently exists that combines these models to produce the data required by statistical methods to robustly estimate species distribution. Consequently, the extent to which machine learning can truly reduce the data processing burden for conservation remains unclear. This study addresses that gap by delivering the first fully automated system for generating species distribution data from raw camera trap video, leveraging deep learning and computer vision in an end-to-end ecological application.

\section{Hypothesis}

This project will harness recent advances in machine learning and computer vision to test a central hypothesis: that computer vision methods can extract metadata from camera trap footage which, when used by statistical models for marked and unmarked populations of chimpanzees, yield abundance and density estimates with precision that \textbf{matches or surpasses} expert-derived metadata.

\section{Aims \& Objectives}

The primary aims of this project are two-fold: \textbf{(A1)} to develop a automated pipeline that extracts rich ecological metadata (annotations) from camera trap videos of chimpanzees. This pipeline will generate structured annotations—including spatio-temporal localisation, species, behaviour, distance-to-camera, and individual identity—required to support advanced population analysis. These outputs will enable abundance and density estimation for unmarked and marked chimpanzee populations using existing statistical approaches; and \textbf{(A2)} provide a rigorous evaluation of whether distribution statistics (e.g., density, abundance) produced using the automated pipeline match or exceed the precision of those derived from traditional expert-based manual annotation. This validation will determine the pipeline’s effectiveness as a scalable and reliable tool for ecological monitoring.

The project will be delivered through four interdependent workstreams, which together constitute a modular but fully integrated and automated pipeline for ecological analysis (WS1–WS3), accompanied by a robust evaluation protocol (WS4).

\begin{enumerate}[label=\textbf{WS\arabic*:}]
      \item \textbf{Core Data Extraction - Localisation, Species, and Behaviour} \\
            We will develop a deep learning computer vision system to extract spatio-temporal localisation, species category, and behaviour, from raw camera trap footage. These outputs will provide the core \textit{what}, \textit{where}, and \textit{how} data required for both unmarked and marked population analyses.

      \item \textbf{Distance Estimation for CTDS} \\
            We will utilise the core outputs from WS1 in combination with monocular depth estimation methods to extract the metric distance between animals and cameras. This distance information is critical for unmarked abundance and density estimation using camera trap distance sampling (CTDS).

      \item \textbf{Re-Identification for SECR} \\
            We will integrate behavioural context from WS1 and distance cues from WS2 to develop a robust open-set re-identification algorithm. Identification of individuals is essential for enabling marked abundance and density estimation via spatially explicit capture-recapture (SECR) methods.

      \item \textbf{Evaluation Against Expert-Derived Estimates} \\
            To evaluate whether the automated pipeline can \underline{match} human-level performance in ecological inference, we will compare the abundance and density estimates it produces with those derived from expert annotations.

            \begin{itemize}
                  \item For unmarked analyses using CTDS, we will assess whether estimates based on automated annotations—specifically species, behaviour, and animal-to-camera distance—are consistent with those generated from manually annotated datasets.
                  \item For marked analyses using SECR, we will examine whether individual encounter histories produced by the re-identification model yield accurate estimates when processed through standard SECR methods, and how these compare to results based on expert-curated identities.
            \end{itemize}

            To evaluate whether the automated pipeline can \underline{exceed} human-level performance, we will deploy the full system in a camera trap survey at Tai National Park, Côte d’Ivoire — a unique setting where the chimpanzee population is habituated, its distribution is well-known, and previous CTDS and SERC estimates are available. By assessing the degree to which the system-generated metadata yields CTDS and SERC estimates that more closely align with the known distribution than those produced using human-generated metadata, we can provide a strong empirical analysis of whether the system surpasses human-level performance. This rare benchmarking opportunity will facilitate a rigorous assessment of the pipeline’s performance and its scalability as a tool for ecological monitoring.

\end{enumerate}

\section{Data}

This project draws on two primary sources of chimpanzee camera trap data: (\textbf{D1}) \textit{Pan African Programme: The Cultured Chimpanzee (PanAf)} which comprises approximately 20,000 videos of chimpanzees collected from 18 research sites across 15 African countries - all videos contain behaviour annotations \hl{HK-Does PanAf have distance too?}%This dataset serves as a primary source of localisation, identity, and behaviour-labelled footage.
and; (2) (\textbf{D2}) \textit{Wild Chimpanzee Foundation (WCF) Biomonitoring Programme} which comprises more than 500,000 videos of over 30 species, including chimpanzees, collected at $\sim$~2,000 different locations at four national parks in Côte d’Ivoire, Guinea, and Liberia - all videos contain species annotations. % It forms the primary source of data for training species 
Together, these datasets and their associated subsets provide the data needed to deliver on the projects aims (\textbf{A1} and \textbf{A2}).

\textbf{A1 - Model Development}. Below we list the subsets used to support the training and evaluation of task-specific models to infer spatio-temporal localisation, species, behaviour, distance-to-camera, and individual identities from camera trap video:

\begin{itemize}
      \item \textbf{(D1A)} \textit{PanAf-500}: $\sim$~180,000 frames with dense annotations for localisation, bounding boxes, and intra-video identity. Used for pre-training segmentation models in WS1.

      \item \textbf{(D1B)} \textit{PanAf-FGBG}: $\sim$~81,000 manually annotated frames with spatio-temporal segmentation masks. Used for fine-tuning segmentation models in WS1.

      \item \textbf{(D1C)} \textit{PanAf-Identity}: A manually curated subset \hl{HK-How many videos? Who is doing annotations? Can we have access?} of the main dataset containing over 1,000 chimpanzees with confirmed individual identities. Used as ground truth for training and evaluating the re-identification model in WS3.

      \item \textbf{(D2A)} \textit{WCF-Reference}: $\sim$~2,000 reference videos (from each location) recorded at known distances (via placards) to support calibration of monocular depth estimation models in WS2.

      \item \textbf{(D2B)} \textit{WCF-Distance}: $\sim$~100,000 videos from the main dataset, manually annotated every 2 seconds with animal-to-camera distance. Used for fine-tuning distance models in WS2.
\end{itemize}

Unless stated otherwise, datasets are partitioned into training, validation, and test sets such that camera trap locations are mutually exclusive across splits. This ensures rigorous out-of-distribution evaluation. Where applicable, splits are stratified to preserve class balance (e.g., across species or behaviours), and all sampling is conducted at the level of individual clips or sequences to avoid data leakage. Model performance is reported as the average across $k$-fold cross-validation ($k=5$).


% TNP/MBNP Behaviour Set: A small, high-quality subset of ~5,000 videos with expert-annotated fine-grained behaviour labels. Used for behaviour classification in WS1.

\textbf{A2 - Ecological Evaluation}. Three other datasets subsets are drawn from camera trap studies in which distribution estimates based on expert-annotated data are already available. These subsets are used to evaluate the performance of the automated pipeline against outputs from real-world ecological analyses, providing a realistic test of the system’s accuracy at operational scale:

\begin{itemize}
      \item \textbf{(D2C)} \textit{WCF-Offset}: $\sim$~100,000 videos collected from four current offset sites (Beauvois, Fello-Sounga, Koumbia and Sinceri) entirely distinct from the locations used for primary model training within the WCF Biomonitoring Programme. Known species, distance, and expert-derived CTDS distribution statistics. Used to evaluate the distance estimation module in WS2.
      \item \textbf{(D2D)} \textit{WCF-Moyen-Bafing National Park (Bakoun \& Koukoutamba)}: $\sim$~4,000 videos collected from a dense network of camera traps, with known individual identities and expert-derived SECR distribution statistics. Used to evaluate the re-identification algorithm in WS3.
      \item \textbf{(D2E)} \textit{WCF-Tai National Park (TNP)}: \hl{X} videos sourced from camera trap surveys within Tai National Park, Cote d'Ivoire (size TBC), featuring footage of a habituated chimpanzee population. For this population, individual identities are known with high confidence, and reliable distribution estimates calculated with \textbf{both} marked (SECR) and unmarked (CTDS) methodologies already exist. Applying the full pipeline (WS1-3) to this data allows for direct comparison of automated results against a \textit{known} distribution.
\end{itemize}

\section*{Workstreams}

Each of the proposed workstreams breaks down into several interdependent tasks. Below, we describe each of them in turn. % Before outlining the workstreams in detail, we first introduce the data splitting protocol, core datasets, evaluation metrics, and other methodological conventions that apply across workstreams. These provide a consistent foundation for all model development and validation.

\section*{WS-1: Core Data Extraction - Localisation, Species, and Behaviour}

This workstream focuses on extracting structured, fine-grained information from camera trap videos to support downstream ecological analysis. It is organised around three core tasks: (1) generating spatio-temporal segmentation masks for individual chimpanzees through segmentation and tracking; (2) classifying the species of tracked individuals; and (3) recognising ecologically significant behaviours, with an emphasis on detecting camera reactivity. % For each objective, we describe the task and its ecological motivation, followed immediately by the methods we will use to achieve it—including model architectures, datasets, training procedures, and evaluation strategies. This integrated structure ensures that the practical implementation of each objective is tightly linked to its scientific rationale.

\textbf{Spatio-Temporal Segmentation}. First, we will generate high-quality masklets using foundation models for segmentation, combined with state-of-the-art tracking algorithms, to produce temporally consistent representations of individual chimpanzees. This enables the isolation of semantically meaningful signals relating to appearance and motion, while suppressing spurious background information and providing intra-video identity tracking. To achieve this, we will train the Segment Anything Models (SAM) on two datasets previously compiled by our lab. During pre-training, SAM-3 will utilise the PanAf-500 which comprises $\sim$~180,000 densely annotated frames with bounding boxes and intra-video IDs (i.e., tracklets). Following pre-training the model will be fine-tuned on the recently published PanAf-FGBG, which contains $\sim$~80,000 frames with dense masks. Tracking and propagation techniques will then be used to create a segmentation mask for future frames (i.e., a spatio-temporal segmentation mask or masklet). We will report performance using metrics for segmentation and tracking (e.g., Intersection over Union, Average Precision, Average Recall, Multiple Object Tracking Accuracy).


\textbf{Species Classification}. Second, we will develop a species classifier that infers fine-grained taxonomic categories directly from the extracted masklets. By focusing on segmented representations of tracked individuals, this approach avoids reliance on background context and enhances generalisability across locations and environments. Taxonomic categorisation is essential for species-level analysis~\cite{}. To achieve this, we will fine-tune state-of-the-art classification models using both image and video-based architectures. We will explore different input modalities—comparing full-frame inputs against segmented inputs (masklets), and static images against temporal sequences. Specifically, we will benchmark models such as Zamba~\cite{}, Bioclip~\cite{}, and SpeciesNet~\cite{} (image-based) alongside 3D CNNs~\cite{} and Transformer-based~\cite{} architectures (video-based). Recent research has cropped inputs are preferable to full-frames in image-models~\cite{}. However, our own internal experimentation has shown full-frame video models are on par with image models that make use of cropped inputs, highlighting the potential for temporal information to further improve species classification. Therefore, we plan to test the hypothesise that utilising masklet (i.e., segemented vs. cropped) sequences as inputs will provide additional performance benefits \hl{(see Sec. X)}. Finally, we will explore model consensus for quantifying uncertainty and identifying sources of error; preliminary results highight the potential in this approach \hl{(see Tab. X)}. Model development will leverage the WCF Biomonitoring dataset, a rich training corpus of over 500,000 species-annotated camera trap videos collected over eight years from four national parks in West Africa: Tai National Park (Côte d’Ivoire), Pinselli-Soyah-Sabouyah and Moyen Bafing National Parks (Guinea), and Grebo-Krahn National Park (Liberia).


\textbf{Behaviour Recognition}. Third, we will build a video-based behaviour recognition model to identify ecologically significant behaviours, with a focus on detecting camera reactivity. Automating this task is essential, as failing to account for reactivity—where animal behaviour is altered by the presence of the camera—can lead to substantial overestimates in species abundance~\cite{}. Yet, such biases are rarely addressed systematically or at scale. To support this objective, we will develop and evaluate a video-based classification model capable of recognising a range of behaviours, with reactivity as a core target. We will investigate the impact of input type on model performance, comparing standard full-frame video inputs to cropped sequences derived from masklets. We will also explore methods for explicitly integrating pose information into the model pipeline, motivated by prior research showing pose is highly informative~\cite{} while background context can degrade performance~\cite{}. To do this, we utilise 20,000 videos with expert annotations spanning 20 behaviour classes—including reactivity—from the Pan African Programme.

Together, these three components extract what species is present, where it is located, and how it is behaving—delivering the structured information needed in WS2–WS3.

\section*{WS-2: Automated Distance Estimation with Calibration}

This workstream aims to predict metric distances from identified animals (output from WS1) to the camera, a step essential for density estimation using CTDS. Our approach involves two key stages: first, we will leverage state-of-the-art depth estimation models efficient alongside training-free calibration techniques; second, we will perform large-scale fine-tuning on a broader annotated dataset to maximize the precision of these distance estimates.

\textbf{Training-free Calibration}. First, we will evaluate a set of pre-trained monocular depth estimation models using training-free calibration techniques. We will assess several state-of-the-art depth estimation architectures—such as Dense Prediction Transformers, Depth Anything, and Sapiens—augmented with efficient, training-free calibration techniques developed for wildlife camera trap scenarios~\cite{}. To enable calibration, we leverage reference videos from WCF-Reference, collected from over 2,000 individual camera locations, featuring placards placed at several precisely measured distances from the camera \hl{(see Fig. X)}. These methods will allow us to adapt existing models originally trained on large-scale datasets to wildlife-specific footage without requiring expensive model retraining. Model performance will be evaluated using standard depth estimation metrics such as Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). The best candidate model from the calibration stage will be fine-tuned to further improve performance. For this, we will leverage the WCF-Distance dataset, a substantial annotated dataset: 100,000 videos that have been manually annotated with estimated distances at regular time intervals (every two seconds), guided by reference videos. The model architecture identified in the calibration stage will then be fine-tuned using this dataset to enhance metric precision beyond what can be achieved through calibration alone.

\section*{WS-3: Re-Identification using Distance and Behaviour}

% \subsection*{Motivation}

% Marked methods such as Spatial Event Capture-Recapture (SECR) can produce highly precise abundance estimates, but they are often impractical in wild populations due to the difficulty and cost of reliably identifying individuals. Traditional approaches to individual ID typically require years of habituation or complex genetic sampling. Re-
% identifying individuals directly from camera trap footage remains challenging due to image quality issues—such as occlusion, variable distances, and inconsistent lighting—as well as behavioural variability, all of which make manual annotation both unstable and labour-intensive. Although automated re-identification is a highly active area of
% ecological research [cite Hahn, 2022], existing methods still face substantial limitations [cite Pollock, 2025], often relying narrowly on appearance cues such as facial features or coat patterns.

This workstream focuses on developing a novel chimpanzee re-identification algorithm tailored to camera trap data, with the ultimate goal of enabling individual-based abundance estimation using SECR.

\textbf{Re-Identiﬁcation using Distance and Behaviour}. Our approach will account specifically for factors known to affect re-identification accuracy: the behaviour (WS1) and distance of the animal from the camera (WS2). Specifically, we will achieve this by uniquely leveraging the behaviour classifications from WS1 and robust distance estimates developed in WS2 as integral components of the re-identification model. We hypothesize that incorporating distance information (to overcome spatial/scale variance) and behavioural context (via contrastive approaches that learn identity across differing behaviours) will lead to a more robust and accurate re-identification system compared to existing methods. %This addresses the critical need for reliable, automated individual identification to facilitate marked population studies. 
To train the re-identification model, we will utilise the extensive camera trap dataset from the Pan African Programme: The Cultured Chimpanzee. This dataset comprises 20,000 videos from 18 sites across 15 countries, and includes over 1,000 individually identified chimpanzees along with detailed metadata, including distance and behaviour. This provides a large-scale foundation with known individual identities to train and validate the model’s ability to learn robust identity features. We will report results using standard re-identification metrics, including Rank-1 accuracy and mean Average Precision (mAP).

% For model development, we will implement the novel approach outlined in the objective, focusing on architectures that can effectively integrate the distance information from WS2 and behaviour classifications from WS1. This will likely involve exploring [Placeholder: Specify potential model architectures, e.g., specific types of deep metric learning, attention mechanisms, graph networks] combined with contrastive learning strategies (sampling pairs of the same individual exhibiting different behaviours or at different distances) and calibration mechanisms informed by distance estimates to mitigate spatial variance effects on appearance. [Placeholder: Add more specific details on the proposed algorithm, loss functions, and calibration mechanism here].

\section*{WS-4: Evaluation Against Expert-Derived Estimates}

The final workstream evaluates whether the fully automated pipeline developed in WS1–WS3 can replicate or exceed the accuracy of expert-derived ecological estimates. Specifically, we assess the pipeline’s ability to support both density and abundance estimation using unmarked (CTDS) and marked (SERC) analyses. This involves two complementary evaluations: comparison to expert annotations on existing data, and deployment in a uniquely well-understood field site.


\textbf{Unmarked Analysis - CTDS}. First, we will assess the performance of the automated system in the context of unmarked population analysis. For this, we utilise the pipeline developed in WS1 and WS2—comprising models for localisation, species identification, behaviour classification (WS1), and distance estimation (WS2). The pipeline will process video footage to generate a comprehensive set of outputs for each animal detection — aggregated over 2-second windows — including spatio-temporal localisation, species identity, behaviour classification, and metric distance. Then, these automatically generated annotations will be aggreated and passed to the CTDS framework as inputs. To evaluate the real-world performance of our automated system, we will compare the abundance and density estimates produced using these automated inputs against those obtained from an identical CTDS analysis using fully human-annotated species, behaviour, and distance data. This direct comparison will provide a rigorous assessment of the pipeline’s accuracy and its potential to scale ecological monitoring.

% Potential for additional evaluation: for CTDS, abundance and density estimates, as well as the underlying detection functions, produced using the automated pipeline will be directly compared...

\textbf{Marked Analysis - SERC}. Second, we will evaluate the performance of the re-identification model developed in WS3 in a marked-population context using SECR. While the PanAf dataset is suitable for training, it lacks associated abundance/density estimates needed to assess the algorithm’s utility in a real ecological analysis. Therefore, we will use data from Moyen Bafing National Park (MBNP), Guinea, where dense camera trap grids were deployed at two 100 km² sites—Bakoun and Koukoutamba—between 2018 and 2020. Over 4,000 chimpanzee videos were collected, and expert ecologists manually identified 227 individuals in Bakoun and 207 in Koukoutamba, producing SECR-based abundance and density estimates. We will apply our re-identification model to this dataset and use the automatically generated encounter histories as inputs to an SECR analysis. By comparing the resulting automated estimates against the expert-derived SECR outputs, we will assess the model’s utility for individual-based ecological inference.

\textbf{Evaluation of re-ID against expert-based CTDS, SERC and habituation}. Finally, to determine whether the system can exceed the performance of human-derived data, we will deploy it within a camera trap survey conducted in the Tai National Park, Cote d’Ivoire. This population is habituated,
meaning individual identities and population distribution statistics are well known (with high confidence). Additionally, estimates derived with both marked (SECR) and unmarked (CTDS) methods using expert-derived metadata already exist. Therefore, applying the automated system here allows for direct comparison against established population estimates derived from multiple methodologies. By evaluating the degree to which the system-generated metadata yields CTDS and SERC estimates that more closely align with the known distribution than those produced using human generated metadata, we will provide a strong empirical analysis of whether the system surpasses human-level performance. This deployment will also facilitate further analysis, investigating the quantitative relationship between distance estimation accuracy (from WS2) and re-identification performance, and how uncertainties in both propagate to influence the precision of final estimates of population distribution.

\section{Significance}

This project represents a major advance in conservation technology through three key contributions:
(\textbf{S1}) \textit{Pioneering a Fully-Automated Biodiversity Monitoring System} — this study will deliver the first fully automated system for generating species distribution data from raw camera trap video using both marked and unmarked statistical approaches. In addition to distribution data, the system will produce key biodiversity metrics as by-products, including species richness, and Shannon and Simpson diversity indices;
(\textbf{S2}) \textit{Transformational Efficiency and Real-World Applicability} — preliminary benchmarks indicate that processing times will be reduced from \textit{X year} to \textit{Y days} (see Fig.~X), marking a substantial advance in conservation ecology. This acceleration enables scalable, near-real-time analysis and paves the way for a new generation of integrated biomonitoring methods. Designed for out-of-distribution conditions, the system will be broadly applicable across conservation contexts once open-sourced and published;
(\textbf{S3}) \textit{Unlocking Archival Data and Addressing Bias in Ape Conservation Rationale} — by accounting for behavioural bias, the system will enable robust reanalysis of archival data, potentially revealing an even more urgent conservation outlook for the earth’s remaining great apes. This will also represent the first analytical study to quantify the effects of camera reactivity behaviour on abundance estimates derived from marked statistical methods, and may stimulate urgently needed conservation interventions.

\bibliographystyle{plain}
\bibliography{sample}

\end{document}
\section*{NERC: Pushing the frontiers of environmental research}

% Wrap below in \nerc
\nerc{
\section*{Details}
\subsection*{Application title}
The application title should begin with NERC/NSF/FAPESP/FNR if submitted under one of those international partnerships
\subsection*{Summary (550 words)}
This will be used to help select the most appropriate assessors for your application.
In plain English, provide a summary we can use to identify the most suitable experts to assess your application.
We may make this summary publicly available on external-facing websites, so make it suitable for a variety of readers, for example:
% Conver to list
\begin{itemize}
    \item opinion-formers
    \item policymakers
    \item the public
    \item the wider research community
\end{itemize}
Clearly describe your proposed work in terms of:
\begin{itemize}
    \item context
    \item the challenge the project addresses
    \item aims and objectives
    \item potential applications and benefits
\end{itemize}
}

\section*{Summary (approx. 550 words)}

\subsection*{Lay-person Draft}

\textbf{Context}. Biodiversity is declining at an unprecedented rate, with species disappearing faster than at any point in the past tens of millions of years. Great apes are among the most threatened: all species are now classified as Endangered or Critically Endangered, and most populations are still shrinking. Western chimpanzees, for example, have declined by more than 80\% in the last 20 years and have already vanished from several countries in West Africa. These species are not only of profound ecological value—playing key roles in their ecosystems—but are also central to many scientific disciplines, including biology, anthropology, and psychology.

Despite substantial conservation investment, these efforts have not yet succeeded in reversing population declines. There is a critical need for more effective, data-driven conservation strategies, and recent advances in artificial intelligence (AI) and computer vision offer untapped potential to support this goal. By using AI to accelerate and improve wildlife monitoring, we have an opportunity to make timely, evidence-based decisions that better protect endangered species like chimpanzees.

\textbf{The Challenge}. Current conservation planning is hindered by a lack of reliable, up-to-date information about animal populations. Although camera traps—motion-sensitive video cameras—are now widely used to monitor wildlife, they generate huge volumes of video footage that are extremely time-consuming to review manually. This creates a serious bottleneck: video is being collected far faster than it can be analysed, delaying or preventing the use of crucial data in conservation decision-making.

As a result, statistical tools used to estimate population size or distribution are often limited by data availability or rely on costly, expert-led annotation. Furthermore, AI models that can detect species, behaviours, or individuals from images or video are often developed in isolation, without integration into real ecological workflows. There is currently no end-to-end system that transforms raw camera trap footage into the kinds of ecological insights—like population size estimates—that conservation practitioners need.

This project tackles these gaps by building the first complete AI system designed to produce species distribution and abundance data directly from raw camera trap video. It combines cutting-edge deep learning with ecological modelling to automate what currently requires extensive manual effort.

\textbf{Aims \& Objectives}. The aim is to develop and test AI tools that extract ecological information from camera trap videos and link it directly to statistical models for estimating chimpanzee population size and density. The specific objectives are:

\begin{itemize}
    \item Develop state-of-the-art AI systems that can accurately identify species, age, sex, behaviour, location, distance from camera, and individual identity from camera trap videos.
    \item Create an integrated system where these components work together, learning cross-dependencies (e.g., how behaviour or distance affects identification accuracy).
    \item Rigorously evaluate the system using large, annotated datasets from known chimpanzee populations to test performance under real-world conservation conditions.
\end{itemize}

\textbf{Potential Applications and Benefits}. Although the focus is on great apes, the methods developed will be adaptable to a wide range of species and ecosystems. This work supports a vision of environmental science that is digitally connected, scalable, and timely. By reducing reliance on manual annotation and accelerating the use of video data, the project will enable more accurate and responsive biodiversity monitoring. The system will inform conservation planning, support evidence-based policy, and help develop more effective interventions. It also contributes to UK leadership in AI for environmental science and builds tools that can be reused across global conservation efforts. 

\subsection*{Technical Draft}

\textbf{Aims \& Objectives}. The aim of this project is to develop and evaluate visual AI methods that extract ecological metadata directly from camera trap footage and integrate this information with statistical models of both ‘marked’ and ‘unmarked’ chimpanzee populations. The goal is to produce accurate estimates of abundance and density under real-world conservation conditions, matching or surpassing expert-driven, annotation-based approaches while removing the need for time-consuming manual video review. This will be validated using integrated population models applied to well-characterised populations with known demographics, drawing on the world’s largest visual repository of great ape camera trap footage. The key objectives are three-fold: (1) to produce benchmarked AIs that extract rich sets of independently derived ecological metadata from camera trap videos of chimpanzees with state-of-the-art accuracy – including species, age, sex, behaviour, spatio-temporal localisation, distance-to-camera, and individual identity – required to support advanced population analysis; (2) to engineer AIs that can be practically co-optimised across its components, in which ecological estimates are consistent, wherein and cross-metadata correlations (e.g., behaviour affecting abundance estimation accuracy [27] or distance affecting re-identification accuracy) are learned and exploited; and (3) to provide a rigorous evaluation on large, existing and conservation-relevant population data answering our hypothesis at various system development stages.

\textbf{Potential Applications \& Benefits}. The methods developed in this project, though focused on great apes to ensure feasibility, are designed to be broadly applicable across species, ecosystems, and monitoring contexts. By integrating camera trap footage with advanced AI pipelines and statistical modelling, the project aligns with NERC’s vision for a digitally connected environmental science ecosystem—linking data capture, analysis, and decision-making. The approach offers a scalable, automated alternative to manual annotation, enabling accurate and efficient biodiversity monitoring. It exemplifies the embedding of AI and sensing technologies into core environmental research, delivering outputs that support evidence-based conservation policy and adaptive ecosystem management. Additionally, the tools and frameworks produced will be transferable across taxa and regions, contributing to capacity building and positioning the UK at the forefront of AI-enabled environmental science.

\nerc{
\section*{Vision (1100 words)}
What are you hoping to achieve with your proposed work?
Explain how your proposed work:
\begin{itemize}
    \item is of excellent quality and importance within or beyond the field(s) or area(s)  
    \item has the potential to advance current understanding, or generate new knowledge, thinking or discovery within or beyond the field or area
    \item is timely, given current trends, context, and needs
    \item impacts world-leading research, society, the economy or the environment  
\end{itemize}
Within this section we also expect you to:
\begin{itemize}
    \item identify the potential direct or indirect benefits and who the beneficiaries might be 
\end{itemize}
References may be included within this section.
You may demonstrate elements of your responses in visual form if relevant. You should:
\begin{itemize}
    \item use images sparingly and only to convey important information that cannot easily be put into words
    \item insert each new image onto a new line
    \item provide a descriptive legend for each image immediately underneath it (this counts towards your word limit)
    \item ensure that files are smaller than 5MB and in JPEG, JPG, JPE, JFI, JIF, JFIF, PNG, GIF, BMP or WEBP format
    \item Your application may be rejected if images are provided without a descriptive legend in the text box, or are used to replace text that could be input into the text box.
\end{itemize}
}


\textbf{Why Leverhulme?}. To build and transform our understanding of how humanity’s latest great invention (Artificial Intelligence) can protect biodiversity and help safeguard the survival of our closest relatives (great apes) sits at the heart of both, this proposal as well as my overall research vision. I believe, given the transferability of the project concept to countless other species, the research has potential to pioneer a new engineering-centred sensor-to-estimate AI avenue for biodiversity research and its general methodologies. Concentrating on a component-based approach and one endangered family of primates gives the project distinct focus and allows combining tangible, conservation-relevant outcomes with a radical research agenda within the project. It brings together data and methods from a diverse range of fields spanning the computational, physical and biological sciences, including deep learning and computer vision, methods in ecology and statistical population modelling, innovative sensing, imageomics, as well as species conservation – all scientific areas in which I have published. This broad blend of disciplines forms the testbed for the proposed novel end-to-end approach for AI-driven species conservation. Beyond that, I believe it can inspire future research in computational and global change biology. This project therefore represents a unique opportunity to support a truly interdisciplinary researcher and his team, to showcase how AI can be reimagined, and how it can develop maximum impact for good to help better monitor and protect the natural world. For these reasons I believe that this work represents an excellent fit to the remit, mission, and ethos of the Leverhulme Trust.

\textbf{Motivation, Timeliness, \& Importance}. Global animal diversity is declining at unprecedented rates~\cite{Tuia2022} -- orders-of-magnitude faster than in tens of millions of years~\cite{Ceballos2020}. In particular, all great ape species are now listed as Endangered or Critically Endangered, and all but one subspecies, are still declining in numbers~\cite{Hockings2015}. Despite substantial investment in conservation efforts~\cite{Bettinger2021}, western chimpanzees have declined by 80\%+ in the last two decades~\cite{Kuhl2017} having been extirpated from at least three countries (Benin, Burkina Faso and Togo)~\cite{Ginn2013,Campbell2015} and several other populations are on the edge (e.g., Ghana)~\cite{Kuhl2017}. Given these are effective flagship, umbrella, indicator and keystone species, are essential for human well-being and survival~\cite{MEA2005}, and of intense scientific interest to many scientific disciplines (e.g., biology, anthropology, psychology and their many subfields), there is a critical need -- but also untapped AI technology potential~\cite{Tuia2022} -- to protect them more effectively at this point in time~\cite{Paine1969,Tutin1991,Clucas2008}.

\textbf{Background, Significance, \& Research Vision}. Despite considerable research attention~\cite{Marshall2016}, relatively little effort is focused on actual conservation ~\cite{Bezanson2019}. Alarmingly, there is limited scientific evidence for the effectiveness of existing conservation strategies~\cite{Junker2020,Junker2017} driven by inability to assess population responses to conservation measures due to the absence of robust monitoring systems~\cite{Witmer2005,Junker2020}. This would require rapid  processing of sensor data to quantify shifts in species distribution and enable timely interventions. Despite pressing necessity, such systems have yet to be developed partly explaining why truly up-to-date population estimates are largely outstanding for great apes~\cite{Campbell2008, Walsh2008, Plumptre2010, Morgan2011, ArcusFoundation2014, Humle2016}. Camera traps have now been widely adopted for studying great apes~\cite{Burton2015, Tuia2022}. While easing data availability, the sheer video volumes generated have created a severe data processing bottleneck. So far, laborious expert annotations are routinely required to fuel primary 'marked' or 'unmarked' statistical frameworks that derive population estimates. In fact, the generation of video footage now significantly outpaces downstream processes needed to analyse it~\cite{Hebblewhite2010, Burton2015, Tuia2022, Pollock2025}. As a result, automated methods for timely data processing are now amongst the largest emerging needs of ecologists in general~\cite{Nathan2022} and great ape conservationists in particular~\cite{}. Moreover, models are often not integrated or even evaluated against actual ecological conservation pipelines. Instead, AI species classification, individual identification, distance estimation, and behaviour extraction are performed out of context - no system currently exists that combines these aspects to fuel statistical methods robustly for fast and accuracy-enhanced species distribution estimates. This study addresses the above gaps by attempting to build a first end-to-end AI system that generates species distribution data directly from raw camera trap videos, leveraging deep learning and computer vision in its ecological application space.

\textbf{Is of excellent quality and importance within or beyond the field(s) or area(s)}.

\textbf{Has the potential to advance current understanding, or generate new knowledge, thinking or discovery within or beyond the field or area}.

This project represents a major advance in conservation technology through three key contributions:
(\textbf{S1}) \textit{Pioneering a Fully-Automated Biodiversity Monitoring System} — this study will deliver the first fully automated system for generating species distribution data from raw camera trap video using both marked and unmarked statistical approaches. In addition to distribution data, the system will produce key biodiversity metrics as by-products, including species richness, and Shannon and Simpson diversity indices;
(\textbf{S2}) \textit{Transformational Efficiency and Real-World Applicability} — preliminary benchmarks indicate that processing times will be reduced from \textit{X year} to \textit{Y days} (see Fig.~X), marking a substantial advance in conservation ecology. This acceleration enables scalable, near-real-time analysis and paves the way for a new generation of integrated biomonitoring methods. Designed for out-of-distribution conditions, the system will be broadly applicable across conservation contexts once open-sourced and published;
(\textbf{S3}) \textit{Unlocking Archival Data and Addressing Bias in Ape Conservation Rationale} — by accounting for behavioural bias, the system will enable robust reanalysis of archival data, potentially revealing an even more urgent conservation outlook for the earth’s remaining great apes. This will also represent the first analytical study to quantify the effects of camera reactivity behaviour on abundance estimates derived from marked statistical methods, and may stimulate urgently needed conservation interventions.
(\textbf{S3}) \textit{Vision-only Distribution Prediction} — Therefore, statistical approaches are used to “estimate both species abundance and its spatiotemporal trends from a statistically representative sample of individuals drawn from the entire population” (Pollock et al., 2025) although “estimates based on extrapolation from a small number of point counts have large uncertainties and can fail to capture the spatiotemporal variation in ecological relationships, resulting in erroneous predictions or extrapolations” (Tuia et al., 2022, Rollinson et al., 2021). \ob{Should say something like - a major drawback of current statistical approaches is that they are based on extrapolation from a small number of point counts, which have large uncertainties and can fail to capture the spatiotemporal variation in ecological relationships, resulting in erroneous predictions or extrapolations. The project will address this gap by developing a system that can generate species distribution data from raw camera trap video - and in the latter stages utilise in a world-first approach visual information \textit{alone} - rather than statistical methods - to estimate both marked and unmarked abundance directly to improve the accuracy of population estimates. For elusive and cryptic species, evaluation is often difficult but unique access to the worlds largest respoitory of great ape camera trap data alongside \textit{known} - rather than estimated - populations provides a unique opportunity to validate the system.}

Prestonian shortfall stuff?

\textbf{Is timely, given current trends, context, and needs}.

\textbf{Impacts world-leading research, society, the economy or the environment}.

\textbf{Identify the potential direct or indirect benefits and who the beneficiaries might be}. The methods developed in this project, though focused on great apes to ensure feasibility, are designed to be broadly applicable across species, ecosystems, and monitoring contexts. By integrating camera trap footage with advanced AI pipelines and statistical modelling, the project aligns with NERC’s vision for a digitally connected environmental science ecosystem—linking data capture, analysis, and decision-making. The approach offers a scalable, automated alternative to manual annotation, enabling accurate and efficient biodiversity monitoring. It exemplifies the embedding of AI and sensing technologies into core environmental research, delivering outputs that support evidence-based conservation policy and adaptive ecosystem management. Additionally, the tools and frameworks produced will be transferable across taxa and regions, contributing to capacity building and positioning the UK at the forefront of AI-enabled environmental science.


\nerc{
\section*{Approach (2750 words)}
How are you going to deliver your proposed work?
Explain how you have designed your work so that it:
\begin{itemize}
    \item is effective and appropriate to achieve your objectives  
    \item is feasible, and comprehensively identifies any risks to delivery and how they will be managed
    \item uses a clearly written and transparent methodology (if applicable) 
    \item summarises the previous work and describes how this will be built upon and progressed (if applicable)  
    \item will maximise translation of outputs into outcomes and impacts   
    \item describes how your, and if applicable your team’s, research environment (in terms of the place and relevance to the project) will contribute to the success of the work
\end{itemize}
Within this section we also expect you to:
\begin{itemize}
    \item demonstrate access to the appropriate services, facilities, infrastructure, or equipment to deliver the proposed work  
    \item provide a detailed and comprehensive project plan, including milestones and timelines in the form of a Gantt chart or similar
\end{itemize}
References may be included within this section.
You may demonstrate elements of your responses in visual form if relevant. You should:
\begin{itemize}
    \item use images sparingly and only to convey important information that cannot easily be put into words
    \item insert each new image onto a new line
    \item provide a descriptive legend for each image immediately underneath it (this counts towards your word limit)
    \item ensure that files are smaller than 5MB and in JPEG, JPG, JPE, JFI, JIF, JFIF, PNG, GIF, BMP or WEBP format
\end{itemize}
Your application may be rejected if images are provided without a descriptive legend in the text box, or are used to replace text that could be input into the text box.
}

\section*{Methodological Outline}

\textbf{summarises the previous work and describes how this will be built upon and progressed (if applicable)}



\textbf{Is feasible, and comprehensively identiﬁes any risks to delivery and how they will be managed}

Component-based Timeline \& Risk-reduction. We propose five interdependent WorkStreams that form a modular pipeline for ecological analysis: WS1: core metadata extraction; WS2: monocular distance estimation and unmarked camera trap distance sampling (CTDS) [29]; WS3: individual identification and spatially explicit capture-recapture (SECR) for marked populations [30]; WS4: integration and co-optimisation; and WS5: robust system evaluation using defined benchmarks. This design (see Fig. 3) significantly reduces project risk by allowing partial outputs (e.g., WS1-3) to be developed and shared independently with the community, while isolating higher-risk unified AI integration (WS4-5).

\textbf{Uses a clearly written and transparent methodology} \& \textbf{provides a detailed and comprehensive project plan, including milestones and timelines in the form of a Gantt chart or similar}

\textbf{Datasets \& Study Populations}. AI techniques rely on large, richly annotated datasets for training and fine-tuning. This project will have full access to two sources of such chimpanzee camera trap data, which together form the world’s largest visual great ape repository of their kind and fully support the project's aims and hypothesis testing: (D1) The Pan African Programme: The Cultured Chimpanzee (PanAf) Datasets including [2, 6] comprise ~20k videos from 15 countries. All videos are annotated with species and behaviour; ~5k cover spatio-temporal localisation, and ~10k are identity, sex and age-annotated for over 1,000 individuals; (D2) Wild Chimpanzee Foundation (WCF) Datasets include >500k videos from ~2k locations, including one reference video per location with placards at known distances, 100,000 videos annotated with animal-to-camera distances, and ~5,000 videos identity-annotated for 500 individual chimpanzees with associated age and sex information. Several subsets of this data come from sites where population statistics are known via habituation and have been independently estimated using SECR and CTDS.

\ob{\textbf{Leverhulme Methodology}. Methods are ambitious, varied, and tailored to each workstream.} 

\section*{WS-1: Core Data Extraction - Localisation, Species, and Behaviour}

This workstream focuses on extracting structured, fine-grained information from camera trap videos to support downstream ecological analysis. It is organised around three core tasks: (1) generating spatio-temporal segmentation masks for individual chimpanzees through segmentation and tracking; (2) classifying the species of tracked individuals; and (3) recognising ecologically significant behaviours, with an emphasis on detecting camera reactivity.

\textbf{Spatio-Temporal Segmentation}. First, we will generate high-quality masklets using foundation models for segmentation, combined with state-of-the-art tracking algorithms, to produce temporally consistent representations of individual chimpanzees. This enables the isolation of semantically meaningful signals relating to appearance and motion, while suppressing spurious background information and providing intra-video identity tracking. To achieve this, we will train the Segment Anything Models (SAM) on two datasets previously compiled by our lab. During pre-training, SAM-3 will utilise the PanAf-500 which comprises $\sim$~180,000 densely annotated frames with bounding boxes and intra-video IDs (i.e., tracklets). Following pre-training the model will be fine-tuned on the recently published PanAf-FGBG, which contains $\sim$~80,000 frames with dense masks. Tracking and propagation techniques will then be used to create a segmentation mask for future frames (i.e., a spatio-temporal segmentation mask or masklet). We will report performance using metrics for segmentation and tracking (e.g., Intersection over Union, Average Precision, Average Recall, Multiple Object Tracking Accuracy).


\textbf{Species Classification}. Second, we will develop a species classifier that infers fine-grained taxonomic categories directly from the extracted masklets. By focusing on segmented representations of tracked individuals, this approach avoids reliance on background context and enhances generalisability across locations and environments. Taxonomic categorisation is essential for species-level analysis~\cite{}. To achieve this, we will fine-tune state-of-the-art classification models using both image and video-based architectures. We will explore different input modalities—comparing full-frame inputs against segmented inputs (masklets), and static images against temporal sequences. Specifically, we will benchmark models such as Zamba~\cite{}, Bioclip~\cite{}, and SpeciesNet~\cite{} (image-based) alongside 3D CNNs~\cite{} and Transformer-based~\cite{} architectures (video-based). Recent research has cropped inputs are preferable to full-frames in image-models~\cite{}. However, our own internal experimentation has shown full-frame video models are on par with image models that make use of cropped inputs, highlighting the potential for temporal information to further improve species classification. Therefore, we plan to test the hypothesise that utilising masklet (i.e., segemented vs. cropped) sequences as inputs will provide additional performance benefits \hl{(see Sec. X)}. Finally, we will explore model consensus for quantifying uncertainty and identifying sources of error; preliminary results highight the potential in this approach \hl{(see Tab. X)}. Model development will leverage the WCF Biomonitoring dataset, a rich training corpus of over 500,000 species-annotated camera trap videos collected over eight years from four national parks in West Africa: Tai National Park (Côte d’Ivoire), Pinselli-Soyah-Sabouyah and Moyen Bafing National Parks (Guinea), and Grebo-Krahn National Park (Liberia).


\textbf{Behaviour Recognition}. Third, we will build a video-based behaviour recognition model to identify ecologically significant behaviours, with a focus on detecting camera reactivity. Automating this task is essential, as failing to account for reactivity—where animal behaviour is altered by the presence of the camera—can lead to substantial overestimates in species abundance~\cite{}. Yet, such biases are rarely addressed systematically or at scale. To support this objective, we will develop and evaluate a video-based classification model capable of recognising a range of behaviours, with reactivity as a core target. We will investigate the impact of input type on model performance, comparing standard full-frame video inputs to cropped sequences derived from masklets. We will also explore methods for explicitly integrating pose information into the model pipeline, motivated by prior research showing pose is highly informative~\cite{} while background context can degrade performance~\cite{}. To do this, we utilise 20,000 videos with expert annotations spanning 20 behaviour classes—including reactivity—from the Pan African Programme.

Together, these three components extract what species is present, where it is located, and how it is behaving—delivering the structured information needed in WS2–WS3.

\ob{\textbf{NERC - Aims \& Objectives} - We will develop a deep learning computer vision system to extract spatio-temporal localisation, species category, and behaviour, from raw camera trap footage. These outputs will provide the core \textit{what}, \textit{where}, and \textit{how} data required for both unmarked and marked population analyses.}

\ob{\textbf{Leverhulme Methodology} - WS1 will prepare data and extract state-of-the-art estimates of what species is present, where it is located, and how it is behaving in standardised format to fuel WS2-4. Using D1, we will fine-tune foundation models (e.g., SAM3 [31, 32]) to generate spatio-temporal masklets [32] for each appearing ape, augment species classifiers (e.g., Bioclip [36]) with age and sex prediction heads to produce robust taxonomic categorisation, and adapt behaviour recognition systems [2, 34] to focus on ecologically significant behaviours (i.e., camera reactivity [27]).}

\section*{WS-2: Automated Distance Estimation with Calibration}

This workstream aims to predict metric distances from identified animals (output from WS1) to the camera, a step essential for density estimation using CTDS. Our approach involves two key stages: first, we will leverage state-of-the-art depth estimation models efficient alongside training-free calibration techniques; second, we will perform large-scale fine-tuning on a broader annotated dataset to maximize the precision of these distance estimates.

\textbf{Training-free Calibration}. First, we will evaluate a set of pre-trained monocular depth estimation models using training-free calibration techniques. We will assess several state-of-the-art depth estimation architectures—such as Dense Prediction Transformers, Depth Anything, and Sapiens—augmented with efficient, training-free calibration techniques developed for wildlife camera trap scenarios~\cite{}. To enable calibration, we leverage reference videos from WCF-Reference, collected from over 2,000 individual camera locations, featuring placards placed at several precisely measured distances from the camera \hl{(see Fig. X)}. These methods will allow us to adapt existing models originally trained on large-scale datasets to wildlife-specific footage without requiring expensive model retraining. Model performance will be evaluated using standard depth estimation metrics such as Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). The best candidate model from the calibration stage will be fine-tuned to further improve performance. For this, we will leverage the WCF-Distance dataset, a substantial annotated dataset: 100,000 videos that have been manually annotated with estimated distances at regular time intervals (every two seconds), guided by reference videos. The model architecture identified in the calibration stage will then be fine-tuned using this dataset to enhance metric precision beyond what can be achieved through calibration alone.

\ob{\textbf{NERC - Aims \& Objectives} - We will utilise the core outputs from WS1 in combination with monocular depth estimation methods to extract the metric distance between animals and cameras. This distance information is critical for unmarked abundance and density estimation using camera trap distance sampling (CTDS)}

\ob{\textbf{Leverhulme Methodology} - WS2 aims to estimate metric distances for WS1-detected animals in preparation for CTDS. We will first calibrate existing monocular depth estimation models [35] using D2's reference videos before fine-tuning the best performing model on D2's animal-to-camera distance videos.}

\section*{WS-3: Re-Identification using Distance and Behaviour}

This workstream focuses on developing a novel chimpanzee re-identification algorithm tailored to camera trap data, with the ultimate goal of enabling individual-based abundance estimation using SECR.

\textbf{Re-Identiﬁcation using Distance and Behaviour}. Our approach will account specifically for factors known to affect re-identification accuracy: the behaviour (WS1) and distance of the animal from the camera (WS2). Specifically, we will achieve this by uniquely leveraging the behaviour classifications from WS1 and robust distance estimates developed in WS2 as integral components of the re-identification model. We hypothesize that incorporating distance information (to overcome spatial/scale variance) and behavioural context (via contrastive approaches that learn identity across differing behaviours) will lead to a more robust and accurate re-identification system compared to existing methods. To train the re-identification model, we will utilise the extensive camera trap dataset from the Pan African Programme: The Cultured Chimpanzee. This dataset comprises 20,000 videos from 18 sites across 15 countries, and includes over 1,000 individually identified chimpanzees along with detailed metadata, including distance and behaviour. This provides a large-scale foundation with known individual identities to train and validate the model’s ability to learn robust identity features. We will report results using standard re-identification metrics, including Rank-1 accuracy and mean Average Precision (mAP).

\ob{\textbf{Leverhulme Methodology} - Utilising D1's identity annotated videos, WS3 aims to develop a new SECR-compatible, full-body chimpanzee re-identification approach that explicitly exploits appearance, behaviour and distance in unison. This makes resolution dependency of re-ID accuracy explicit and, for the first time, considers individual ape behaviour/gait signatures as relevant training signals.}

\section*{WS-4: Evaluation Against Expert-Derived Estimates}

\ob{\textbf{Leverhulme Methodology} - WS4 attempts a high-risk, high reward strategy to exploit ecological estimate and wildlife-metadata correlations – never used at scale for deep network optimisation before. Techniques such as co-optimisation, network fusion, weight sharing, latent space alignment, and federated learning may be utilised to learn correlations (e.g., age, sex and behaviours may all corroborate an ape’s ID) with cost functions that enforce consistencies of statistical outcomes (e.g. CTDS vs. SECR on suitable sub-datasets). The latter mimics aspects of integrated population modelling computationally and may link ‘marked’ and ‘unmarked’ pipelines end-to-end. Using D2, WS5 benchmarks outputs from WS2–WS4 and quantifies hypotheses by comparing AI and expert-derived estimates of population statistics. }


\section*{WS-5: Evaluation Against Expert-Derived Estimates}

The final workstream evaluates whether the fully automated pipeline developed in WS1–WS3 can replicate or exceed the accuracy of expert-derived ecological estimates. Specifically, we assess the pipeline’s ability to support both density and abundance estimation using unmarked (CTDS) and marked (SERC) analyses. This involves two complementary evaluations: comparison to expert annotations on existing data, and deployment in a uniquely well-understood field site.

\textbf{Unmarked Analysis - CTDS}. First, we will assess the performance of the automated system in the context of unmarked population analysis. For this, we utilise the pipeline developed in WS1 and WS2—comprising models for localisation, species identification, behaviour classification (WS1), and distance estimation (WS2). The pipeline will process video footage to generate a comprehensive set of outputs for each animal detection — aggregated over 2-second windows — including spatio-temporal localisation, species identity, behaviour classification, and metric distance. Then, these automatically generated annotations will be aggreated and passed to the CTDS framework as inputs. To evaluate the real-world performance of our automated system, we will compare the abundance and density estimates produced using these automated inputs against those obtained from an identical CTDS analysis using fully human-annotated species, behaviour, and distance data. This direct comparison will provide a rigorous assessment of the pipeline’s accuracy and its potential to scale ecological monitoring.

\textbf{Marked Analysis - SERC}. Second, we will evaluate the performance of the re-identification model developed in WS3 in a marked-population context using SECR. While the PanAf dataset is suitable for training, it lacks associated abundance/density estimates needed to assess the algorithm’s utility in a real ecological analysis. Therefore, we will use data from Moyen Bafing National Park (MBNP), Guinea, where dense camera trap grids were deployed at two 100 km² sites—Bakoun and Koukoutamba—between 2018 and 2020. Over 4,000 chimpanzee videos were collected, and expert ecologists manually identified 227 individuals in Bakoun and 207 in Koukoutamba, producing SECR-based abundance and density estimates. We will apply our re-identification model to this dataset and use the automatically generated encounter histories as inputs to an SECR analysis. By comparing the resulting automated estimates against the expert-derived SECR outputs, we will assess the model’s utility for individual-based ecological inference.

\textbf{Evaluation of re-ID against expert-based CTDS, SERC and habituation}. Finally, to determine whether the system can exceed the performance of human-derived data, we will deploy it within a camera trap survey conducted in the Tai National Park, Cote d’Ivoire. This population is habituated,
meaning individual identities and population distribution statistics are well known (with high confidence). Additionally, estimates derived with both marked (SECR) and unmarked (CTDS) methods using expert-derived metadata already exist. Therefore, applying the automated system here allows for direct comparison against established population estimates derived from multiple methodologies. By evaluating the degree to which the system-generated metadata yields CTDS and SERC estimates that more closely align with the known distribution than those produced using human generated metadata, we will provide a strong empirical analysis of whether the system surpasses human-level performance. This deployment will also facilitate further analysis, investigating the quantitative relationship between distance estimation accuracy (from WS2) and re-identification performance, and how uncertainties in both propagate to influence the precision of final estimates of population distribution.

\ob{\textbf{Leverhulme Methodology} - To test if the system matches expert performance, estimates are compared against existing CTDS or SECR outputs; to test if it exceeds expert performance, it is deployed at habituated sites with fully known population statistics.}




\textbf{Demonstrate access to the appropriate services, facilities, infrastructure, or equipment to deliver the proposed work}

Resources, Collaborators, Dissemination, and Transferability. Proposed AI methods are cutting edge and require latest supercomputer support as available from the UKs Isambard AI platform – with the University of Bristol sponsoring additional supercomputer time (BluePebble \ BlueCrystal). A global team of ecology experts will provide regular and multidisciplinary feedback to the core team (i.e., applicant, co-applicant, RA and PhD student (see Fig. 3)). Backed by the team’s track record, we will publish ethically anonymised (useless to poachers) and institutionally curated (via data.bris.ac.uk) datasets/code and target top-end venues across disciplines incl. CVPR, TREE, IJCV, Nature Comms, Methods in Ecology and Evolution, but also specialist work- shops such as CV4Animals and CamTrapAI to report to our global AI for Wildlife community. Workstreams proposed are in principle structurally applicable to other species and monitoring settings. The focus on great apes solely aids feasibility of this ambitious project which reimagines conservation through a lens of AI engineering. Fig. 3) Brief Project Overview. Visualisation of component-based project structure including staff and workstream flow from datasets to evaluation.











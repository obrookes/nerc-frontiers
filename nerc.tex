\section*{NERC: Pushing the frontiers of environmental research}


\nerc{
\section*{Applicant and team capability to deliver}
Evidence of how you, and if relevant your team, have:
\begin{itemize}
    \item the relevant experience (appropriate to career stage) to deliver the proposed work
    \item the right balance of skills and expertise to cover the proposed work 
    \item the appropriate leadership and management skills to deliver the work and your approach to develop others (appropriate to career stage)
    \item contributed to developing a positive research environment and wider community 
\end{itemize}
}

% Wrap below in \nerc
\nerc{
\section*{Details}
\subsection*{Application title}
The application title should begin with NERC/NSF/FAPESP/FNR if submitted under one of those international partnerships
\subsection*{Summary (550 words)}
This will be used to help select the most appropriate assessors for your application.
In plain English, provide a summary we can use to identify the most suitable experts to assess your application.
We may make this summary publicly available on external-facing websites, so make it suitable for a variety of readers, for example:
% Conver to list
\begin{itemize}
    \item opinion-formers
    \item policymakers
    \item the public
    \item the wider research community
\end{itemize}
Clearly describe your proposed work in terms of:
\begin{itemize}
    \item context
    \item the challenge the project addresses
    \item aims and objectives
    \item potential applications and benefits
\end{itemize}
}

\section*{Summary (approx. 550 words)}

\subsection*{Lay-person Draft}

\textbf{Context}. Biodiversity is declining at an unprecedented rate, with species disappearing faster than at any point in the past tens of millions of years. Great apes are among the most threatened: all species are now classified as Endangered or Critically Endangered, and most populations are still shrinking. Western chimpanzees, for example, have declined by more than 80\% in the last 20 years and have already vanished from several countries in West Africa. These species are not only of profound ecological value—playing key roles in their ecosystems—but are also central to many scientific disciplines, including biology, anthropology, and psychology.

Despite substantial conservation investment, these efforts have not yet succeeded in reversing population declines. There is a critical need for more effective, data-driven conservation strategies, and recent advances in artificial intelligence (AI) and computer vision offer untapped potential to support this goal. By using AI to accelerate and improve wildlife monitoring, we have an opportunity to make timely, evidence-based decisions that better protect endangered species like chimpanzees.

\textbf{Challenges at the Interface of Conservation \& AI}. Conservation planning is currently hindered by several interlinked challenges. Endangered species like chimpanzees are typically sparsely distributed, elusive in their behaviour, and inhabit difficult or unstable environments—such as dense tropical forests or regions experiencing political unrest—which makes data collection extremely challenging. Even when data is successfully gathered, camera traps generate huge volumes of video footage that require laborious manual annotation to extract ecological metadata such as species identity, individual ID, and behaviour. This creates a major bottleneck: video is collected much faster than it can be reviewed, delaying the use of critical information in conservation decisions. Even once this enormous manual effort is completed, population estimates still rely on statistical methods that extrapolate from small numbers of detections. These approaches suffer from large uncertainties and often fail to capture spatial or temporal variation in ecological relationships—such as how behaviour or environmental context influences detectability—leading to imprecise or misleading estimates of species abundance and distribution.

In parallel, although AI tools now exist to identify species, behaviours, and even individuals from images or video, they are rarely integrated into end-to-end systems that deliver ecological insights. This is largely because such tools are typically developed in isolation—without consideration of ecological objectives—and are not evaluated against targets such as species distributions or population sizes. Critically, such evaluation is difficult: while it is relatively straightforward to create datasets of individual samples (e.g., video-label pairs for species ID), evaluating performance against ecological targets requires a different structure—each data point is effectively a dataset in itself (e.g., a full collection of camera trap videos linked to a known population size or spatial distribution). Assembling and curating such data is logistically challenging, and rigorous evaluation protocols for this kind of ecological AI system have yet to be defined. These barriers mean current AI tools are rarely tested at the scale or ecological complexity required for real-world conservation, and remain disconnected from the statistical models used to support evidence-based decisions.

\textbf{Aims \& Objectives}. This project tackles these gaps by building the first complete AI system designed to produce population-level estimates directly from raw camera trap video. The specific objectives are:

\begin{itemize}
    \item Develop state-of-the-art AI systems that can accurately identify species, age, sex, behaviour, location, distance from camera, and individual identity from camera trap videos. This responds directly to the need for tools that operate at conservation-relevant scales and complexities, and enables population estimation from raw data without relying on labour-intensive manual annotation.
    \item Create an integrated system where these components are co-optimised and function together, learning cross-dependencies. This integration breaks with the current siloed approach to AI development, supporting more consistent, interpretable, and ecologically grounded metadata, improving the quality of inputs to subsequent population modelling and analysis.
    \item Develop an AI system capable of predicting species distribution statistics directly from raw video, bypassing the need for conventional statistical models. This ambitious objective aims to learn population-level patterns directly, overcoming current barriers around model integration, evaluation complexity, and statistical extrapolation from limited data. Achieving this will require the development of entirely new training strategies and task-level evaluation protocols—extending the scope of AI research to operate meaningfully at ecological scales and targets.
\end{itemize}

Rigorous, ecologically grounded evaluation, is central to all objectives. We will benchmark system components and end-to-end outputs using large, annotated datasets from known chimpanzee populations, enabled by our unique access to the world’s largest repository of great ape camera trap footage. These datasets—many drawn from sites with independently verified population statistics—allow us to test system performance against real-world ecological targets and ensure reliability at the scale and complexity required for practical conservation.

\textbf{Potential Applications and Benefits}. Although the focus is on great apes, the methods developed will be adaptable to a wide range of species and ecosystems. This work supports a vision of environmental science that is digitally connected, scalable, and timely. By reducing reliance on manual annotation and accelerating the use of video data, the project will enable more accurate and responsive biodiversity monitoring. The system will inform conservation planning, support evidence-based policy, and help develop more effective interventions. It also contributes to UK leadership in AI for environmental science and builds tools that can be reused across global conservation efforts. 

% \subsection*{Technical Draft}

% \textbf{Aims \& Objectives}. The aim of this project is to develop and evaluate visual AI methods that extract ecological metadata directly from camera trap footage and integrate this information with statistical models of both ‘marked’ and ‘unmarked’ chimpanzee populations. The goal is to produce accurate estimates of abundance and density under real-world conservation conditions, matching or surpassing expert-driven, annotation-based approaches while removing the need for time-consuming manual video review. This will be validated using integrated population models applied to well-characterised populations with known demographics, drawing on the world’s largest visual repository of great ape camera trap footage. The key objectives are three-fold: (1) to produce benchmarked AIs that extract rich sets of independently derived ecological metadata from camera trap videos of chimpanzees with state-of-the-art accuracy – including species, age, sex, behaviour, spatio-temporal localisation, distance-to-camera, and individual identity – required to support advanced population analysis; (2) to engineer AIs that can be practically co-optimised across its components, in which ecological estimates are consistent, wherein and cross-metadata correlations (e.g., behaviour affecting abundance estimation accuracy [27] or distance affecting re-identification accuracy) are learned and exploited; and (3) to provide a rigorous evaluation on large, existing and conservation-relevant population data answering our hypothesis at various system development stages.

% \textbf{Potential Applications \& Benefits}. The methods developed in this project, though focused on great apes to ensure feasibility, are designed to be broadly applicable across species, ecosystems, and monitoring contexts. By integrating camera trap footage with advanced AI pipelines and statistical modelling, the project aligns with NERC’s vision for a digitally connected environmental science ecosystem—linking data capture, analysis, and decision-making. The approach offers a scalable, automated alternative to manual annotation, enabling accurate and efficient biodiversity monitoring. It exemplifies the embedding of AI and sensing technologies into core environmental research, delivering outputs that support evidence-based conservation policy and adaptive ecosystem management. Additionally, the tools and frameworks produced will be transferable across taxa and regions, contributing to capacity building and positioning the UK at the forefront of AI-enabled environmental science.

\nerc{
\section*{Vision (1100 words)}
What are you hoping to achieve with your proposed work?
Explain how your proposed work:
\begin{itemize}
    \item is of excellent quality and importance within or beyond the field(s) or area(s)  
    \item has the potential to advance current understanding, or generate new knowledge, thinking or discovery within or beyond the field or area
    \item is timely, given current trends, context, and needs
    \item impacts world-leading research, society, the economy or the environment  
\end{itemize}
Within this section we also expect you to:
\begin{itemize}
    \item identify the potential direct or indirect benefits and who the beneficiaries might be 
\end{itemize}
References may be included within this section.
You may demonstrate elements of your responses in visual form if relevant. You should:
\begin{itemize}
    \item use images sparingly and only to convey important information that cannot easily be put into words
    \item insert each new image onto a new line
    \item provide a descriptive legend for each image immediately underneath it (this counts towards your word limit)
    \item ensure that files are smaller than 5MB and in JPEG, JPG, JPE, JFI, JIF, JFIF, PNG, GIF, BMP or WEBP format
    \item Your application may be rejected if images are provided without a descriptive legend in the text box, or are used to replace text that could be input into the text box.
\end{itemize}
}

\textbf{Introduction}. We aim to establish a new paradigm in ecological monitoring by building the first AI system that directly produces population-level ecological estimates from raw camera trap video. This project will bring together data and methods from a diverse range of fields spanning the computational, physical and biological sciences, including deep learning and computer vision, methods in ecology and statistical population modelling, as well as species conservation to deliver a novel end-to-end approach for AI-driven species conservation and enable accurate, real-time monitoring of endangered species at the spatial and temporal scales required to inform effective conservation.

% Current approaches face multiple, compounding limitations: camera traps now generate vast volumes of video data, but extracting meaningful ecological insights requires laborious manual annotation — a process that creates a major bottleneck and delays conservation decisions. Even when annotation is completed, population estimates are typically derived from small numbers of detections using statistical extrapolation, which introduces substantial uncertainty and often fails to reflect ecological complexity. While AI systems now exist for tasks such as species or individual identification, these are usually developed in isolation, without consideration of ecological objectives, and are rarely evaluated against metrics that matter for conservation — such as species distributions or population sizes. 


\textbf{Timeliness, Context \& Need}. Global biodiversity is declining at a rate unprecedented in tens of millions of years, and great apes are among the most acutely affected: all species are listed as Endangered or Critically Endangered, and nearly all subspecies are in decline. Western chimpanzees, for example, have declined by over 80\% in just two decades, disappearing from multiple countries in West Africa. Despite decades of scientific attention and substantial investment in conservation, there is limited empirical evidence that existing strategies are effective. A major reason is the lack of robust, scalable systems to monitor population changes in real time and at scale.

Camera traps have now become central to wildlife monitoring, offering a non-invasive method to capture behaviour, distribution, and demography. However, they produce video footage at scales that far exceed our capacity to process it. Expert annotation remains essential for extracting ecological metadata such as species, individual identity, behaviour, and distance to camera—data that in turn feed statistical population models. Yet this annotation is time-consuming and expensive, and the volume of incoming video already vastly outpaces our ability to analyse it. Even when annotation is completed, population estimates still rely on extrapolation from sparse detections, using statistical methods that can introduce substantial uncertainty. At the same time, although AI systems have been developed for species recognition, re-identification, distance estimation, and behaviour classification, these tools are typically created in isolation. They are rarely integrated or evaluated within full ecological pipelines, and almost never benchmarked against the ultimate conservation targets they aim to support—such as population abundance, demographic structure, or species distribution. As a result, automated methods for timely data processing are now amongst the largest emerging needs of ecologists in general~\cite{Nathan2022}, and of conservation practitioners in particular.

This project is timely because these challenges intersect with a moment of real opportunity. Recent advances in deep learning—especially in video understanding, foundation models, behaviour classification, and spatial reasoning—now make it technically feasible to build an integrated system that learns from video at scale and delivers ecological outputs of direct conservation relevance. Our proposal directly addresses the key priorities of the NERC Digital Strategy by enabling digitally connected, AI-driven environmental science, and aligns with the “Pushing the Frontiers” call by undertaking high-risk, interdisciplinary research with transformative potential. By eliminating key bottlenecks and reshaping how ecological data is analysed and used, this work answers an urgent need with a uniquely timely solution.

% Timeliness - key points
% Biodiversity is declining faster than ever, and conservation efforts have failed to reverse this trend.
% Camera traps are generating vast quantities of video, but existing processing workflows are not scalable.
% AI capabilities have rapidly advanced but remain underused and unintegrated in ecological practice.
% There is global momentum and policy focus on digital, data-driven conservation (e.g., UN Global Biodiversity Framework) as well as from UKRI's Digital Strategy etc.

\textbf{Advancing Understanding, Knowledge, Thinking or Discovery}. This project advances knowledge by fundamentally rethinking how ecological information is extracted, structured, and interpreted from wildlife video. It marks a shift from using AI as a narrow tool for automating isolated tasks—such as species identification or behaviour detection—to deploying it as an integrated, system-level framework capable of learning ecological patterns directly from raw data. This enables a new mode of scientific inference, where population-level outcomes such as species distribution or abundance are not estimated via downstream statistical models, but are instead learned end-to-end using structured visual and behavioural signals.

For ecology, this work opens up new analytical possibilities: continuous, automated monitoring of demographic and behavioural dynamics at scales and resolutions previously inaccessible. It challenges the traditional separation between data collection and population estimation, instead embedding inference directly into the sensing process. This enables researchers to ask questions about how detectability varies with behaviour, how environmental context influences demographic structure, or how individual movement patterns relate to population density—questions that are difficult to address using sparse, manually derived observations.

In artificial intelligence, the project introduces novel training and evaluation regimes tailored to ecological goals. These include supervision at the population or site level, loss functions that enforce statistical consistency across outputs, and architectures that learn ecological interdependencies. Such innovations extend the scope of AI research into domains that require interpretability, structural constraint, and outcome fidelity—offering a new benchmark for AI systems tasked with producing scientific insight, not just prediction accuracy.

By integrating these advances, the project also contributes to a broader rethinking of what ecological monitoring means in the digital age. It moves beyond sampling towards continuous inference; beyond manual workflows to AI-native systems; and beyond siloed modelling pipelines to integrated, adaptable infrastructures for environmental insight. This conceptual shift is relevant not only to conservation, but to other fields grappling with sensor-rich, data-scarce settings—such as climate science, agriculture, and sustainability policy.

% Advances - key points
% Pioneers a new paradigm in ecological monitoring: direct inference of abundance and distribution from raw video.
% Introduces novel AI training strategies and evaluation frameworks suited to ecological outcomes, not just prediction accuracy.
% Enables understanding of how animal characteristics (e.g., age, sex, behaviour) influence detection and identification.
% Bridges the gap between AI and population ecology, creating cross-disciplinary methodological advances.

\textbf{Quality \& Importance within or beyond the field(s) or area(s)}. This project is scientifically ambitious and methodologically rigorous, representing a step change in how artificial intelligence is applied to ecological monitoring. While automated tools for wildlife video analysis exist, they are typically narrow in scope—focused on isolated tasks such as species identification or individual recognition—and developed without reference to ecological inference. By contrast, this proposal brings together a full suite of AI capabilities, integrates them into a coherent system, and evaluates them in terms of their ability to support robust population-level ecological estimates. It is the first project to explicitly co-optimise and evaluate AI components against ecological targets such as species distribution and abundance.

The research builds on recent breakthroughs in visual AI, including behaviour recognition, monocular depth estimation, foundation models, and multi-task learning. These are adapted and extended for ecological use-cases, introducing novel elements such as:

\begin{itemize}
    \item Cross-metadata learning, where outputs like behaviour, age, and distance are jointly optimised to support more consistent downstream inference;
    \item Ecological consistency constraints, to ensure internal coherence of outputs;
    \item Task-level evaluation protocols, where models are assessed based on their ability to recover known population statistics (e.g. from SECR or CTDS models), not just per-frame or video-level accuracy.
\end{itemize}

These contributions are scientifically significant both within and beyond ecology. For AI, the project develops new training strategies that go beyond standard supervised learning, incorporating weak supervision from site-level ecological data. For ecology, it shifts the analytical paradigm from manual, sample-based inference to automated, system-level estimation. Together, these represent a new class of scientific workflow: fully AI-native ecological modelling.

The project is enabled by access to the world’s largest and most richly annotated datasets of great ape video. These include the PanAf dataset (~20,000 videos from 15 countries, annotated with species, identity, age, sex, and behaviour), and the WCF dataset (>500,000 videos from ~2,000 locations, with known camera distances and independently verified population statistics). These data resources allow us to evaluate the system on realistic, conservation-relevant tasks and at scales previously unattainable.

The work will be delivered by a team combining world-leading expertise in machine learning, ecological statistics, and great ape conservation. The proposal is underpinned by close collaboration with field-based conservation partners, ensuring ecological realism in problem design and evaluation. This interdisciplinary strength, combined with rigorous methods and a uniquely well-suited data foundation, makes the proposed work of exceptional scientific quality. It stands to set a new benchmark for integrated AI systems in environmental science.

% Combines cutting-edge AI (e.g., foundation models, co-optimisation, end-to-end learning) with real-world ecological applications.
% Addresses a major, globally recognised challenge: biodiversity decline and failure of current conservation strategies.
% Responds directly to urgent needs identified by conservationists (efficient processing of massive camera trap datasets).
% Builds on globally unique data resources (e.g., PanAf and WCF datasets) with known population ground truths.

\textbf{Impact on World-leading Research, Society, the Economy or the Environment}. This project will enhance the precision and responsiveness of conservation decision-making by enabling real-time, automated population monitoring at scale. It significantly reduces the cost and time required for ecological analysis, delivering immediate societal and economic value to conservation agencies, NGOs, and wildlife managers. By pioneering scalable AI methods tailored to environmental science, the work reinforces UK leadership in digital conservation and ecological AI. Its modular design allows adaptation across species and ecosystems, offering wide-reaching environmental benefits and global applicability. All models, datasets, and benchmarks will be openly shared, creating lasting infrastructure to accelerate future research and innovation.


Research impact will be delivered through the creation of open-access tools, models, and datasets that enable others to replicate, extend, or adapt our approach. By benchmarking AI systems against ground-truthed ecological outcomes and publishing evaluation protocols, we establish new standards for ecological AI research. These resources will benefit a growing global community working at the intersection of AI and environmental science, including contributors to initiatives like CV4Animals, CamTrapAI, and Earth Species Project.

Societal and conservation impact stems from the ability to detect population changes more frequently, across larger spatial extents, and at lower cost than current monitoring systems allow. This can improve the effectiveness of interventions, support more dynamic conservation planning, and help secure funding and policy support for endangered species protection. Outputs from this project will be directly relevant to NGOs, wildlife managers, protected area networks, and international monitoring bodies such as the IUCN SSC and Convention on Biological Diversity.

Environmental impact lies in the ability to reduce biodiversity loss through earlier detection of declines and improved understanding of population dynamics. By facilitating real-time feedback on species status—especially for critically endangered taxa like western chimpanzees—the system enhances the timeliness, spatial coverage, and statistical strength of biodiversity assessments. This supports global priorities such as the Kunming–Montreal Global Biodiversity Framework.

Economic impact will be realised through cost and labour savings across the ecological monitoring pipeline. By automating tasks that currently require skilled annotators and field personnel, we reduce the financial and logistical burden of long-term wildlife studies. This enables broader participation in conservation monitoring, particularly in low-resource settings.

Key beneficiaries include:

\begin{itemize}
    \item Conservation organisations (e.g. Wild Chimpanzee Foundation, PanAf Programme)
    \item Wildlife management authorities (e.g. national park agencies, government biodiversity offices)
    \item NGOs and funders seeking evidence-based project evaluation
    \item Academic researchers in machine learning, ecology, and animal behaviour
    \item Policymakers and advisory bodies relying on up-to-date species distribution data
\end{itemize}

Finally, while the project focuses on great apes, its methodology is deliberately generalisable. The models and system architecture will be modular and designed for transfer to other species and ecosystems—whether for large herbivores in savannas, carnivores in boreal forests, or marine species detected via underwater video. The project will publish its outputs under appropriate licenses and promote uptake through open workshops, collaborative benchmarking challenges, and engagement with interdisciplinary research communities.


\textbf{Why Leverhulme?}. To build and transform our understanding of how humanity’s latest great invention (Artificial Intelligence) can protect biodiversity and help safeguard the survival of our closest relatives (great apes) sits at the heart of both, this proposal as well as my overall research vision. I believe, given the transferability of the project concept to countless other species, the research has potential to pioneer a new engineering-centred sensor-to-estimate AI avenue for biodiversity research and its general methodologies. Concentrating on a component-based approach and one endangered family of primates gives the project distinct focus and allows combining tangible, conservation-relevant outcomes with a radical research agenda within the project. It brings together data and methods from a diverse range of fields spanning the computational, physical and biological sciences, including deep learning and computer vision, methods in ecology and statistical population modelling, innovative sensing, imageomics, as well as species conservation – all scientific areas in which I have published. This broad blend of disciplines forms the testbed for the proposed novel end-to-end approach for AI-driven species conservation. Beyond that, I believe it can inspire future research in computational and global change biology. This project therefore represents a unique opportunity to support a truly interdisciplinary researcher and his team, to showcase how AI can be reimagined, and how it can develop maximum impact for good to help better monitor and protect the natural world. For these reasons I believe that this work represents an excellent fit to the remit, mission, and ethos of the Leverhulme Trust.


\textbf{Is of excellent quality and importance within or beyond the field(s) or area(s)}.

\textbf{Has the potential to advance current understanding, or generate new knowledge, thinking or discovery within or beyond the field or area}.

This project represents a major advance in conservation technology through three key contributions:
(\textbf{S1}) \textit{Pioneering a Fully-Automated Biodiversity Monitoring System} — this study will deliver the first fully automated system for generating species distribution data from raw camera trap video using both marked and unmarked statistical approaches. In addition to distribution data, the system will produce key biodiversity metrics as by-products, including species richness, and Shannon and Simpson diversity indices;
(\textbf{S2}) \textit{Transformational Efficiency and Real-World Applicability} — preliminary benchmarks indicate that processing times will be reduced from \textit{X year} to \textit{Y days} (see Fig.~X), marking a substantial advance in conservation ecology. This acceleration enables scalable, near-real-time analysis and paves the way for a new generation of integrated biomonitoring methods. Designed for out-of-distribution conditions, the system will be broadly applicable across conservation contexts once open-sourced and published;
(\textbf{S3}) \textit{Unlocking Archival Data and Addressing Bias in Ape Conservation Rationale} — by accounting for behavioural bias, the system will enable robust reanalysis of archival data, potentially revealing an even more urgent conservation outlook for the earth’s remaining great apes. This will also represent the first analytical study to quantify the effects of camera reactivity behaviour on abundance estimates derived from marked statistical methods, and may stimulate urgently needed conservation interventions.
(\textbf{S3}) \textit{Vision-only Distribution Prediction} — Therefore, statistical approaches are used to “estimate both species abundance and its spatiotemporal trends from a statistically representative sample of individuals drawn from the entire population” (Pollock et al., 2025) although “estimates based on extrapolation from a small number of point counts have large uncertainties and can fail to capture the spatiotemporal variation in ecological relationships, resulting in erroneous predictions or extrapolations” (Tuia et al., 2022, Rollinson et al., 2021). \ob{Should say something like - a major drawback of current statistical approaches is that they are based on extrapolation from a small number of point counts, which have large uncertainties and can fail to capture the spatiotemporal variation in ecological relationships, resulting in erroneous predictions or extrapolations. The project will address this gap by developing a system that can generate species distribution data from raw camera trap video - and in the latter stages utilise in a world-first approach visual information \textit{alone} - rather than statistical methods - to estimate both marked and unmarked abundance directly to improve the accuracy of population estimates. For elusive and cryptic species, evaluation is often difficult but unique access to the worlds largest respoitory of great ape camera trap data alongside \textit{known} - rather than estimated - populations provides a unique opportunity to validate the system.}

Prestonian shortfall stuff?

\textbf{Is timely, given current trends, context, and needs}.

\textbf{Impacts world-leading research, society, the economy or the environment}.

\textbf{Identify the potential direct or indirect benefits and who the beneficiaries might be}. The methods developed in this project, though focused on great apes to ensure feasibility, are designed to be broadly applicable across species, ecosystems, and monitoring contexts. By integrating camera trap footage with advanced AI pipelines and statistical modelling, the project aligns with NERC’s vision for a digitally connected environmental science ecosystem—linking data capture, analysis, and decision-making. The approach offers a scalable, automated alternative to manual annotation, enabling accurate and efficient biodiversity monitoring. It exemplifies the embedding of AI and sensing technologies into core environmental research, delivering outputs that support evidence-based conservation policy and adaptive ecosystem management. Additionally, the tools and frameworks produced will be transferable across taxa and regions, contributing to capacity building and positioning the UK at the forefront of AI-enabled environmental science.


\nerc{
\section*{Approach (2750 words)}
How are you going to deliver your proposed work?
Explain how you have designed your work so that it:
\begin{itemize}
    \item is effective and appropriate to achieve your objectives  
    \item is feasible, and comprehensively identifies any risks to delivery and how they will be managed
    \item uses a clearly written and transparent methodology (if applicable) 
    \item summarises the previous work and describes how this will be built upon and progressed (if applicable)  
    \item will maximise translation of outputs into outcomes and impacts   
    \item describes how your, and if applicable your team’s, research environment (in terms of the place and relevance to the project) will contribute to the success of the work
\end{itemize}
Within this section we also expect you to:
\begin{itemize}
    \item demonstrate access to the appropriate services, facilities, infrastructure, or equipment to deliver the proposed work  
    \item provide a detailed and comprehensive project plan, including milestones and timelines in the form of a Gantt chart or similar
\end{itemize}
References may be included within this section.
You may demonstrate elements of your responses in visual form if relevant. You should:
\begin{itemize}
    \item use images sparingly and only to convey important information that cannot easily be put into words
    \item insert each new image onto a new line
    \item provide a descriptive legend for each image immediately underneath it (this counts towards your word limit)
    \item ensure that files are smaller than 5MB and in JPEG, JPG, JPE, JFI, JIF, JFIF, PNG, GIF, BMP or WEBP format
\end{itemize}
Your application may be rejected if images are provided without a descriptive legend in the text box, or are used to replace text that could be input into the text box.
}

\section*{Methodological Outline}

\textbf{summarises the previous work and describes how this will be built upon and progressed (if applicable)}



\textbf{Is feasible, and comprehensively identiﬁes any risks to delivery and how they will be managed}

Component-based Timeline \& Risk-reduction. We propose five interdependent WorkStreams that form a modular pipeline for ecological analysis: WS1: core metadata extraction; WS2: monocular distance estimation and unmarked camera trap distance sampling (CTDS) [29]; WS3: individual identification and spatially explicit capture-recapture (SECR) for marked populations [30]; WS4: integration and co-optimisation; and WS5: robust system evaluation using defined benchmarks. This design (see Fig. 3) significantly reduces project risk by allowing partial outputs (e.g., WS1-3) to be developed and shared independently with the community, while isolating higher-risk unified AI integration (WS4-5).

\textbf{Uses a clearly written and transparent methodology} \& \textbf{provides a detailed and comprehensive project plan, including milestones and timelines in the form of a Gantt chart or similar}

\textbf{Datasets \& Study Populations}. AI techniques rely on large, richly annotated datasets for training and fine-tuning. This project will have full access to two sources of such chimpanzee camera trap data, which together form the world’s largest visual great ape repository of their kind and fully support the project's aims and hypothesis testing: (D1) The Pan African Programme: The Cultured Chimpanzee (PanAf) Datasets including [2, 6] comprise ~20k videos from 15 countries. All videos are annotated with species and behaviour; ~5k cover spatio-temporal localisation, and ~10k are identity, sex and age-annotated for over 1,000 individuals; (D2) Wild Chimpanzee Foundation (WCF) Datasets include >500k videos from ~2k locations, including one reference video per location with placards at known distances, 100,000 videos annotated with animal-to-camera distances, and ~5,000 videos identity-annotated for 500 individual chimpanzees with associated age and sex information. Several subsets of this data come from sites where population statistics are known via habituation and have been independently estimated using SECR and CTDS.

\ob{\textbf{Leverhulme Methodology}. Methods are ambitious, varied, and tailored to each workstream.} 

\section*{WS-1: Core Data Extraction - Localisation, Species, and Behaviour}

This workstream focuses on extracting structured, fine-grained information from camera trap videos to support downstream ecological analysis. It is organised around three core tasks: (1) generating spatio-temporal segmentation masks for individual chimpanzees through segmentation and tracking; (2) classifying the species of tracked individuals; and (3) recognising ecologically significant behaviours, with an emphasis on detecting camera reactivity.

\textbf{Spatio-Temporal Segmentation}. First, we will generate high-quality masklets using foundation models for segmentation, combined with state-of-the-art tracking algorithms, to produce temporally consistent representations of individual chimpanzees. This enables the isolation of semantically meaningful signals relating to appearance and motion, while suppressing spurious background information and providing intra-video identity tracking. To achieve this, we will train the Segment Anything Models (SAM) on two datasets previously compiled by our lab. During pre-training, SAM-3 will utilise the PanAf-500 which comprises $\sim$~180,000 densely annotated frames with bounding boxes and intra-video IDs (i.e., tracklets). Following pre-training the model will be fine-tuned on the recently published PanAf-FGBG, which contains $\sim$~80,000 frames with dense masks. Tracking and propagation techniques will then be used to create a segmentation mask for future frames (i.e., a spatio-temporal segmentation mask or masklet). We will report performance using metrics for segmentation and tracking (e.g., Intersection over Union, Average Precision, Average Recall, Multiple Object Tracking Accuracy).


\textbf{Species Classification}. Second, we will develop a species classifier that infers fine-grained taxonomic categories directly from the extracted masklets. By focusing on segmented representations of tracked individuals, this approach avoids reliance on background context and enhances generalisability across locations and environments. Taxonomic categorisation is essential for species-level analysis~\cite{}. To achieve this, we will fine-tune state-of-the-art classification models using both image and video-based architectures. We will explore different input modalities—comparing full-frame inputs against segmented inputs (masklets), and static images against temporal sequences. Specifically, we will benchmark models such as Zamba~\cite{}, Bioclip~\cite{}, and SpeciesNet~\cite{} (image-based) alongside 3D CNNs~\cite{} and Transformer-based~\cite{} architectures (video-based). Recent research has cropped inputs are preferable to full-frames in image-models~\cite{}. However, our own internal experimentation has shown full-frame video models are on par with image models that make use of cropped inputs, highlighting the potential for temporal information to further improve species classification. Therefore, we plan to test the hypothesise that utilising masklet (i.e., segemented vs. cropped) sequences as inputs will provide additional performance benefits \hl{(see Sec. X)}. Finally, we will explore model consensus for quantifying uncertainty and identifying sources of error; preliminary results highight the potential in this approach \hl{(see Tab. X)}. Model development will leverage the WCF Biomonitoring dataset, a rich training corpus of over 500,000 species-annotated camera trap videos collected over eight years from four national parks in West Africa: Tai National Park (Côte d’Ivoire), Pinselli-Soyah-Sabouyah and Moyen Bafing National Parks (Guinea), and Grebo-Krahn National Park (Liberia).


\textbf{Behaviour Recognition}. Third, we will build a video-based behaviour recognition model to identify ecologically significant behaviours, with a focus on detecting camera reactivity. Automating this task is essential, as failing to account for reactivity—where animal behaviour is altered by the presence of the camera—can lead to substantial overestimates in species abundance~\cite{}. Yet, such biases are rarely addressed systematically or at scale. To support this objective, we will develop and evaluate a video-based classification model capable of recognising a range of behaviours, with reactivity as a core target. We will investigate the impact of input type on model performance, comparing standard full-frame video inputs to cropped sequences derived from masklets. We will also explore methods for explicitly integrating pose information into the model pipeline, motivated by prior research showing pose is highly informative~\cite{} while background context can degrade performance~\cite{}. To do this, we utilise 20,000 videos with expert annotations spanning 20 behaviour classes—including reactivity—from the Pan African Programme.

Together, these three components extract what species is present, where it is located, and how it is behaving—delivering the structured information needed in WS2–WS3.

\ob{\textbf{NERC - Aims \& Objectives} - We will develop a deep learning computer vision system to extract spatio-temporal localisation, species category, and behaviour, from raw camera trap footage. These outputs will provide the core \textit{what}, \textit{where}, and \textit{how} data required for both unmarked and marked population analyses.}

\ob{\textbf{Leverhulme Methodology} - WS1 will prepare data and extract state-of-the-art estimates of what species is present, where it is located, and how it is behaving in standardised format to fuel WS2-4. Using D1, we will fine-tune foundation models (e.g., SAM3 [31, 32]) to generate spatio-temporal masklets [32] for each appearing ape, augment species classifiers (e.g., Bioclip [36]) with age and sex prediction heads to produce robust taxonomic categorisation, and adapt behaviour recognition systems [2, 34] to focus on ecologically significant behaviours (i.e., camera reactivity [27]).}

\section*{WS-2: Automated Distance Estimation with Calibration}

This workstream aims to predict metric distances from identified animals (output from WS1) to the camera, a step essential for density estimation using CTDS. Our approach involves two key stages: first, we will leverage state-of-the-art depth estimation models efficient alongside training-free calibration techniques; second, we will perform large-scale fine-tuning on a broader annotated dataset to maximize the precision of these distance estimates.

\textbf{Training-free Calibration}. First, we will evaluate a set of pre-trained monocular depth estimation models using training-free calibration techniques. We will assess several state-of-the-art depth estimation architectures—such as Dense Prediction Transformers, Depth Anything, and Sapiens—augmented with efficient, training-free calibration techniques developed for wildlife camera trap scenarios~\cite{}. To enable calibration, we leverage reference videos from WCF-Reference, collected from over 2,000 individual camera locations, featuring placards placed at several precisely measured distances from the camera \hl{(see Fig. X)}. These methods will allow us to adapt existing models originally trained on large-scale datasets to wildlife-specific footage without requiring expensive model retraining. Model performance will be evaluated using standard depth estimation metrics such as Mean Absolute Error (MAE) and Root Mean Square Error (RMSE). The best candidate model from the calibration stage will be fine-tuned to further improve performance. For this, we will leverage the WCF-Distance dataset, a substantial annotated dataset: 100,000 videos that have been manually annotated with estimated distances at regular time intervals (every two seconds), guided by reference videos. The model architecture identified in the calibration stage will then be fine-tuned using this dataset to enhance metric precision beyond what can be achieved through calibration alone.

\ob{\textbf{NERC - Aims \& Objectives} - We will utilise the core outputs from WS1 in combination with monocular depth estimation methods to extract the metric distance between animals and cameras. This distance information is critical for unmarked abundance and density estimation using camera trap distance sampling (CTDS)}

\ob{\textbf{Leverhulme Methodology} - WS2 aims to estimate metric distances for WS1-detected animals in preparation for CTDS. We will first calibrate existing monocular depth estimation models [35] using D2's reference videos before fine-tuning the best performing model on D2's animal-to-camera distance videos.}

\section*{WS-3: Re-Identification using Distance and Behaviour}

This workstream focuses on developing a novel chimpanzee re-identification algorithm tailored to camera trap data, with the ultimate goal of enabling individual-based abundance estimation using SECR.

\textbf{Re-Identiﬁcation using Distance and Behaviour}. Our approach will account specifically for factors known to affect re-identification accuracy: the behaviour (WS1) and distance of the animal from the camera (WS2). Specifically, we will achieve this by uniquely leveraging the behaviour classifications from WS1 and robust distance estimates developed in WS2 as integral components of the re-identification model. We hypothesize that incorporating distance information (to overcome spatial/scale variance) and behavioural context (via contrastive approaches that learn identity across differing behaviours) will lead to a more robust and accurate re-identification system compared to existing methods. To train the re-identification model, we will utilise the extensive camera trap dataset from the Pan African Programme: The Cultured Chimpanzee. This dataset comprises 20,000 videos from 18 sites across 15 countries, and includes over 1,000 individually identified chimpanzees along with detailed metadata, including distance and behaviour. This provides a large-scale foundation with known individual identities to train and validate the model’s ability to learn robust identity features. We will report results using standard re-identification metrics, including Rank-1 accuracy and mean Average Precision (mAP).

\ob{\textbf{Leverhulme Methodology} - Utilising D1's identity annotated videos, WS3 aims to develop a new SECR-compatible, full-body chimpanzee re-identification approach that explicitly exploits appearance, behaviour and distance in unison. This makes resolution dependency of re-ID accuracy explicit and, for the first time, considers individual ape behaviour/gait signatures as relevant training signals.}

\section*{WS-4: Evaluation Against Expert-Derived Estimates}

\ob{\textbf{Leverhulme Methodology} - WS4 attempts a high-risk, high reward strategy to exploit ecological estimate and wildlife-metadata correlations – never used at scale for deep network optimisation before. Techniques such as co-optimisation, network fusion, weight sharing, latent space alignment, and federated learning may be utilised to learn correlations (e.g., age, sex and behaviours may all corroborate an ape’s ID) with cost functions that enforce consistencies of statistical outcomes (e.g. CTDS vs. SECR on suitable sub-datasets). The latter mimics aspects of integrated population modelling computationally and may link ‘marked’ and ‘unmarked’ pipelines end-to-end. Using D2, WS5 benchmarks outputs from WS2–WS4 and quantifies hypotheses by comparing AI and expert-derived estimates of population statistics. }


\section*{WS-5: Evaluation Against Expert-Derived Estimates}

The final workstream evaluates whether the fully automated pipeline developed in WS1–WS3 can replicate or exceed the accuracy of expert-derived ecological estimates. Specifically, we assess the pipeline’s ability to support both density and abundance estimation using unmarked (CTDS) and marked (SERC) analyses. This involves two complementary evaluations: comparison to expert annotations on existing data, and deployment in a uniquely well-understood field site.

\textbf{Unmarked Analysis - CTDS}. First, we will assess the performance of the automated system in the context of unmarked population analysis. For this, we utilise the pipeline developed in WS1 and WS2—comprising models for localisation, species identification, behaviour classification (WS1), and distance estimation (WS2). The pipeline will process video footage to generate a comprehensive set of outputs for each animal detection — aggregated over 2-second windows — including spatio-temporal localisation, species identity, behaviour classification, and metric distance. Then, these automatically generated annotations will be aggreated and passed to the CTDS framework as inputs. To evaluate the real-world performance of our automated system, we will compare the abundance and density estimates produced using these automated inputs against those obtained from an identical CTDS analysis using fully human-annotated species, behaviour, and distance data. This direct comparison will provide a rigorous assessment of the pipeline’s accuracy and its potential to scale ecological monitoring.

\textbf{Marked Analysis - SERC}. Second, we will evaluate the performance of the re-identification model developed in WS3 in a marked-population context using SECR. While the PanAf dataset is suitable for training, it lacks associated abundance/density estimates needed to assess the algorithm’s utility in a real ecological analysis. Therefore, we will use data from Moyen Bafing National Park (MBNP), Guinea, where dense camera trap grids were deployed at two 100 km² sites—Bakoun and Koukoutamba—between 2018 and 2020. Over 4,000 chimpanzee videos were collected, and expert ecologists manually identified 227 individuals in Bakoun and 207 in Koukoutamba, producing SECR-based abundance and density estimates. We will apply our re-identification model to this dataset and use the automatically generated encounter histories as inputs to an SECR analysis. By comparing the resulting automated estimates against the expert-derived SECR outputs, we will assess the model’s utility for individual-based ecological inference.

\textbf{Evaluation of re-ID against expert-based CTDS, SERC and habituation}. Finally, to determine whether the system can exceed the performance of human-derived data, we will deploy it within a camera trap survey conducted in the Tai National Park, Cote d’Ivoire. This population is habituated,
meaning individual identities and population distribution statistics are well known (with high confidence). Additionally, estimates derived with both marked (SECR) and unmarked (CTDS) methods using expert-derived metadata already exist. Therefore, applying the automated system here allows for direct comparison against established population estimates derived from multiple methodologies. By evaluating the degree to which the system-generated metadata yields CTDS and SERC estimates that more closely align with the known distribution than those produced using human generated metadata, we will provide a strong empirical analysis of whether the system surpasses human-level performance. This deployment will also facilitate further analysis, investigating the quantitative relationship between distance estimation accuracy (from WS2) and re-identification performance, and how uncertainties in both propagate to influence the precision of final estimates of population distribution.

\ob{\textbf{Leverhulme Methodology} - To test if the system matches expert performance, estimates are compared against existing CTDS or SECR outputs; to test if it exceeds expert performance, it is deployed at habituated sites with fully known population statistics.}




\textbf{Demonstrate access to the appropriate services, facilities, infrastructure, or equipment to deliver the proposed work}

Resources, Collaborators, Dissemination, and Transferability. Proposed AI methods are cutting edge and require latest supercomputer support as available from the UKs Isambard AI platform – with the University of Bristol sponsoring additional supercomputer time (BluePebble \ BlueCrystal). A global team of ecology experts will provide regular and multidisciplinary feedback to the core team (i.e., applicant, co-applicant, RA and PhD student (see Fig. 3)). Backed by the team’s track record, we will publish ethically anonymised (useless to poachers) and institutionally curated (via data.bris.ac.uk) datasets/code and target top-end venues across disciplines incl. CVPR, TREE, IJCV, Nature Comms, Methods in Ecology and Evolution, but also specialist work- shops such as CV4Animals and CamTrapAI to report to our global AI for Wildlife community. Workstreams proposed are in principle structurally applicable to other species and monitoring settings. The focus on great apes solely aids feasibility of this ambitious project which reimagines conservation through a lens of AI engineering. Fig. 3) Brief Project Overview. Visualisation of component-based project structure including staff and workstream flow from datasets to evaluation.










